{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"MP3_P1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"cells":[{"cell_type":"code","metadata":{"id":"ZVa8FrOLCdyG","executionInfo":{"status":"ok","timestamp":1603298459340,"user_tz":300,"elapsed":23273,"user":{"displayName":"Oswaldo Russián","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64","userId":"02791438348892449348"}},"outputId":"f1163e61-b403-457a-af20-5d915d0da6b2","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import os\n","os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/Startpkg_A3_P1\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ok5KyFleC0pe","executionInfo":{"status":"ok","timestamp":1603298474771,"user_tz":300,"elapsed":38688,"user":{"displayName":"Oswaldo Russián","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64","userId":"02791438348892449348"}},"outputId":"5541febe-38a1-47ff-969c-f46aa5790150","colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["!pip3 install torch torchvision\n","import torch\n","a = torch.Tensor([1]).cuda()\n","print(a)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n","tensor([1.], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5S2u69br-jtL"},"source":["# Assignment 3 : Multi-label Image Classification"]},{"cell_type":"code","metadata":{"id":"vWUFsltP-jtM","executionInfo":{"status":"ok","timestamp":1603298478782,"user_tz":300,"elapsed":42687,"user":{"displayName":"Oswaldo Russián","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64","userId":"02791438348892449348"}}},"source":["import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torchvision import transforms\n","from sklearn.metrics import average_precision_score\n","from PIL import Image, ImageDraw\n","import matplotlib.pyplot as plt\n","from kaggle_submission import output_submission_csv\n","from classifier import *\n","from voc_dataloader import VocDataset, VOC_CLASSES\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rTenz-md-jtQ"},"source":["In this assignment, you train a classifier to do multi-label classificaton on the PASCAL VOC 2007 dataset. The dataset has 20 different classes which can appear in any given image. Your classifier will predict whether each class appears in an image. This task is slightly different from exclusive multiclass classification like the ImageNet competition where only a single most appropriate class is predicted for an image."]},{"cell_type":"markdown","metadata":{"id":"7BpLzqqA-jtR"},"source":["## Reading Pascal Data"]},{"cell_type":"markdown","metadata":{"id":"oqduMECK-jtR"},"source":["### Loading Training Data"]},{"cell_type":"markdown","metadata":{"id":"FvMET21o-jtS"},"source":["In the following cell we will load the training data and also apply some transforms to the data. Feel free to apply more [transforms](https://pytorch.org/docs/stable/torchvision/transforms.html) for data augmentation which can lead to better performance. "]},{"cell_type":"code","metadata":{"id":"XN3zBPAP-jtS","executionInfo":{"status":"ok","timestamp":1603298478784,"user_tz":300,"elapsed":42685,"user":{"displayName":"Oswaldo Russián","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64","userId":"02791438348892449348"}}},"source":["# Transforms applied to the training data\n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                     std= [0.229, 0.224, 0.225])\n","\n","#train_transform = transforms.Compose([\n","#            transforms.Resize(227),\n","#            transforms.CenterCrop(227),\n","#            transforms.ToTensor(),\n","#            normalize\n","#        ])\n","\n","train_transform = transforms.Compose([\n","            transforms.RandomResizedCrop(227),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            normalize\n","        ])"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"cxTxm_JZ-jtV","executionInfo":{"status":"ok","timestamp":1603299752473,"user_tz":300,"elapsed":1316370,"user":{"displayName":"Oswaldo Russián","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64","userId":"02791438348892449348"}}},"source":["ds_train = VocDataset('VOCdevkit_2007/VOC2007/','train',train_transform)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XbQN96Jq-jtY"},"source":["### Loading Validation Data"]},{"cell_type":"markdown","metadata":{"id":"Z-ylk1NG-jtY"},"source":["We will load the test data for the PASCAL VOC 2007 dataset. Do __NOT__ add data augmentation transforms to validation data."]},{"cell_type":"code","metadata":{"id":"GfD5FOFX-jtZ","executionInfo":{"status":"ok","timestamp":1603299752476,"user_tz":300,"elapsed":1316370,"user":{"displayName":"Oswaldo Russián","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64","userId":"02791438348892449348"}}},"source":["# Transforms applied to the testing data\n","test_transform = transforms.Compose([\n","            transforms.Resize(227),\n","            transforms.CenterCrop(227),\n","            transforms.ToTensor(),\n","            normalize,\n","        ])"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"zpwczlgn-jtb","executionInfo":{"status":"ok","timestamp":1603300954937,"user_tz":300,"elapsed":2518825,"user":{"displayName":"Oswaldo Russián","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64","userId":"02791438348892449348"}}},"source":["ds_val = VocDataset('VOCdevkit_2007/VOC2007/','val',test_transform)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JLQ1lfkW-jte"},"source":["### Visualizing the Data\n","\n","PASCAL VOC has bounding box annotations in addition to class labels. Use the following code to visualize some random examples and corresponding annotations from the train set. "]},{"cell_type":"code","metadata":{"id":"HVASQXuf-jte","executionInfo":{"status":"ok","timestamp":1603300964752,"user_tz":300,"elapsed":2528636,"user":{"displayName":"Oswaldo Russián","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64","userId":"02791438348892449348"}},"outputId":"f99a5955-eda5-45a4-9999-37a595dcee4b","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1gko4UkImscXeNVWWLPDs4bwCSY-MJz-v"}},"source":["for i in range(5):\n","    idx = np.random.randint(0, len(ds_train.names)-1)\n","    _imgpath = os.path.join('VOCdevkit_2007/VOC2007/', 'JPEGImages', ds_train.names[idx]+'.jpg')\n","    img = Image.open(_imgpath).convert('RGB')\n","    draw = ImageDraw.Draw(img)\n","    for j in range(len(ds_train.box_indices[idx])):\n","        obj = ds_train.box_indices[idx][j]\n","        draw.rectangle(list(obj), outline=(255,0,0))\n","        draw.text(list(obj[0:2]), ds_train.classes[ds_train.label_order[idx][j]], fill=(0,255,0))\n","    plt.figure(figsize = (10,10))\n","    plt.imshow(np.array(img))"],"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"y7u8gS2a-jth"},"source":["# Classification"]},{"cell_type":"code","metadata":{"id":"bJMWVSC1-jth","executionInfo":{"status":"ok","timestamp":1603300964753,"user_tz":300,"elapsed":2528633,"user":{"displayName":"Oswaldo Russián","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64","userId":"02791438348892449348"}}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"hYObb1VW-jtk","executionInfo":{"status":"ok","timestamp":1603300964754,"user_tz":300,"elapsed":2528631,"user":{"displayName":"Oswaldo Russián","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64","userId":"02791438348892449348"}}},"source":["train_loader = torch.utils.data.DataLoader(dataset=ds_train,\n","                                               batch_size=50, \n","                                               shuffle=True,\n","                                               num_workers=1)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"vbyInake-jtm","executionInfo":{"status":"ok","timestamp":1603300964755,"user_tz":300,"elapsed":2528628,"user":{"displayName":"Oswaldo Russián","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64","userId":"02791438348892449348"}}},"source":["val_loader = torch.utils.data.DataLoader(dataset=ds_val,\n","                                               batch_size=50, \n","                                               shuffle=True,\n","                                               num_workers=1)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"bhrukOuP-jto","executionInfo":{"status":"ok","timestamp":1603300964756,"user_tz":300,"elapsed":2528625,"user":{"displayName":"Oswaldo Russián","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64","userId":"02791438348892449348"}}},"source":["def train_classifier(train_loader, classifier, criterion, optimizer):\n","    classifier.train()\n","    loss_ = 0.0\n","    losses = []\n","    for i, (images, labels, _) in enumerate(train_loader.dataset):\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        logits = classifier(torch.unsqueeze(images, 0))\n","        loss = criterion(logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","        losses.append(loss)\n","    return torch.stack(losses).mean().item()\n","\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"FxOT8hwn-jtq","executionInfo":{"status":"ok","timestamp":1603300964757,"user_tz":300,"elapsed":2528623,"user":{"displayName":"Oswaldo Russián","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64","userId":"02791438348892449348"}}},"source":["def test_classifier(test_loader, classifier, criterion, print_ind_classes=True):\n","    classifier.eval()\n","    losses = []\n","    with torch.no_grad():\n","        y_true = np.zeros((0,21))\n","        y_score = np.zeros((0,21))\n","        for i, (images, labels, _) in enumerate(test_loader.dataset):\n","            images, labels = images.to(device), labels.to(device)\n","            logits = classifier(torch.unsqueeze(images, 0))\n","            y_true = np.concatenate((y_true, labels.cpu().numpy()[np.newaxis,:]), axis=0)\n","            y_score = np.concatenate((y_score, logits.cpu().numpy()), axis=0)\n","            loss = criterion(logits, labels)\n","            losses.append(loss.cpu().numpy())\n","        aps = []\n","        # ignore first class which is background\n","        for i in range(1, y_true.shape[1]):\n","            ap = average_precision_score(y_true[:, i], y_score[:, i])\n","            if print_ind_classes:\n","                print('-------  Class: {:<12}     AP: {:>8.4f}  -------'.format(VOC_CLASSES[i], ap))\n","            aps.append(ap)\n","        \n","        mAP = np.mean(aps)\n","        test_loss = np.mean(losses)\n","        print('mAP: {0:.4f}'.format(mAP))\n","        print('Avg loss: {}'.format(test_loss))\n","        \n","    return mAP, test_loss, aps"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jx-zPgVT-jtt"},"source":["## Improve the network \n","\n","The network you are given as is will allow you to reach around 0.3 mAP. You need to build two new networks and compare them to the baseline. There are a variety of different approaches you may try:\n","\n","* Network architecture changes\n","    * Number of layers: try adding layers to make your network deeper\n","    * Layer size: try changing the filter size (channel size, kernel size, etc)\n","    * FC layers: Change the number of FC layers\n","    * Batch normalization: adding batch norm between layers will likely give you a significant performance increase\n","    * Residual connections: as you increase the depth of your network, you will find that having residual connections like those in ResNet architectures will be helpful\n","* Optimizer: Instead of plain SGD, you may want to add a learning rate schedule, add momentum, or use one of the other optimizers you have learned about like Adam. Check the `torch.optim` package for other optimizers\n","* Data augmentation: You may use the `torchvision.transforms` module to try adding random resized crops and horizontal flips of the input data. Check `transforms.RandomResizedCrop` and `transforms.RandomHorizontalFlip` for this\n","* Epochs: Once you have found a generally good hyperparameter setting try training for more epochs\n","* Loss function: You might want to add weighting to the `MultiLabelSoftMarginLoss` for classes that are less well represented or experiment with a different loss function\n","\n"]},{"cell_type":"code","metadata":{"id":"K0RkDbYE-jtt","executionInfo":{"status":"ok","timestamp":1603300964758,"user_tz":300,"elapsed":2528620,"user":{"displayName":"Oswaldo Russián","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64","userId":"02791438348892449348"}}},"source":["classifier = ClassifierA().to(device)\n","# You can can use this function to reload a network you have already saved previously\n","#classifier.load_state_dict(torch.load('voc_classifier.pth'))"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"R0evmonA-jtw","executionInfo":{"status":"ok","timestamp":1603300964761,"user_tz":300,"elapsed":2528619,"user":{"displayName":"Oswaldo Russián","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64","userId":"02791438348892449348"}}},"source":["criterion = nn.MultiLabelSoftMarginLoss()\n","#optimizer = torch.optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9)\n","optimizer = torch.optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9, weight_decay = 0.001)\n","#optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"MHOsTA8L-jty","outputId":"f41122d4-9f20-4f99-d756-8fa3d28a972b","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Training the Classifier\n","NUM_EPOCHS = 150  # Change later for more epochs up to 150\n","TEST_FREQUENCY = 5\n","\n","for epoch in range(1, NUM_EPOCHS+1):\n","    print(\"Starting epoch number \" + str(epoch))\n","    train_loss = train_classifier(train_loader, classifier, criterion, optimizer)\n","    print(\"Loss for Training on Epoch \" +str(epoch) + \" is \"+ str(train_loss))\n","    if(epoch%TEST_FREQUENCY==0):\n","        mAP_val, val_loss, _ = test_classifier(val_loader, classifier, criterion)\n","        print('Evaluating classifier')\n","        print(\"Mean Precision Score for Testing on Epoch \" +str(epoch) + \" is \"+ str(mAP_val))\n","        "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Starting epoch number 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iuHDlaUE-jt0"},"source":["# Save the classifier network\n","# Suggestion: you can save checkpoints of your network during training and reload them later\n","torch.save(classifier.state_dict(), './voc_classifierA.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a7y9gpuh-jt3"},"source":["# Evaluate on test set\n","\n"]},{"cell_type":"code","metadata":{"id":"T5hOSB2J-jt3"},"source":["ds_test = VocDataset('VOCdevkit_2007/VOC2007test','test', test_transform)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=ds_test,\n","                                               batch_size=50, \n","                                               shuffle=False,\n","                                               num_workers=1)\n","\n","mAP_test, test_loss, test_aps = test_classifier(test_loader, classifier, criterion)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QNeoYVjq-jt5"},"source":["output_submission_csv('my_solution_ClassifierA_150.csv', mAP_test)"],"execution_count":null,"outputs":[]}]}