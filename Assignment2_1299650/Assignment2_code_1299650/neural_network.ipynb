{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 23034,
     "status": "ok",
     "timestamp": 1601351265686,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "g7AwHFk3V5OK",
    "outputId": "d2a918ea-7465-4579-ab0a-0aaca5eb61f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "import os\n",
    "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/Startpkg_A2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3500,
     "status": "ok",
     "timestamp": 1601351271485,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "hyDJdw6ZVMfN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.data_process import get_CIFAR10_data\n",
    "from models.neural_net import NeuralNetwork\n",
    "from kaggle_submission import output_submission_csv\n",
    "\n",
    "######### If not using Colab, you may skip these setup #########\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "####################### End of setup ###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2X0UNAlVMfQ"
   },
   "source": [
    "# Loading CIFAR-10\n",
    "Now that you have implemented a neural network that passes gradient checks and works on toy data, you will test your network on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 15839,
     "status": "ok",
     "timestamp": 1601351306835,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "im4GJQ0mVMfR"
   },
   "outputs": [],
   "source": [
    "# You can change these numbers for experimentation\n",
    "# For submission be sure they are set to the default values \n",
    "TRAIN_IMAGES = 49000\n",
    "VAL_IMAGES = 1000\n",
    "TEST_IMAGES = 5000  # Default is 5000, do not modify this for your submission. \n",
    "\n",
    "data = get_CIFAR10_data(TRAIN_IMAGES, VAL_IMAGES, TEST_IMAGES)\n",
    "X_train, y_train = data['X_train'], data['y_train']\n",
    "X_val, y_val = data['X_val'], data['y_val']\n",
    "X_test, y_test = data['X_test'], data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCM-79cKVMfU"
   },
   "source": [
    "# Train a network\n",
    "To train our network we will use SGD. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplying it by a decay rate.\n",
    "\n",
    "You can try different numbers of layers and also the different activation functions that you implemented on the CIFAR-10 dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "executionInfo": {
     "elapsed": 26747,
     "status": "ok",
     "timestamp": 1601351337611,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "ikAh5uodVMfU",
    "outputId": "a38223b1-f69c-4030-9032-59cdf0726e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 2.360263\n",
      "iteration 100 / 1000: loss 2.171381\n",
      "iteration 200 / 1000: loss 2.022783\n",
      "iteration 300 / 1000: loss 2.017632\n",
      "iteration 400 / 1000: loss 1.983689\n",
      "iteration 500 / 1000: loss 1.900510\n",
      "iteration 600 / 1000: loss 1.885977\n",
      "iteration 700 / 1000: loss 1.860290\n",
      "iteration 800 / 1000: loss 1.836469\n",
      "iteration 900 / 1000: loss 1.893836\n",
      "Validation accuracy:  0.375\n"
     ]
    }
   ],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "num_layers = 3\n",
    "hidden_size = 120\n",
    "hidden_sizes = [hidden_size]*(num_layers-1)\n",
    "num_classes = 10\n",
    "learning_rate = 0.05\n",
    "learning_rate_decay = 0.95\n",
    "net = NeuralNetwork(input_size, hidden_sizes, num_classes, num_layers, nonlinearity='sigmoid')\n",
    "\n",
    "# Train the network\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=1000, batch_size=200,\n",
    "            learning_rate=learning_rate, learning_rate_decay=learning_rate_decay,\n",
    "            reg=0.00, verbose=True)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJKHH8JGVMfX"
   },
   "source": [
    "# Graph loss and train/val accuracies\n",
    "\n",
    "Examining the loss graph along with the train and val accuracy graphs should help you gain some intuition for the hyperparameters you should try in the hyperparameter tuning below. It should also help with debugging any issues you might have with your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1601351374034,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "oPJGtB7YVMfX",
    "outputId": "c86e65a8-969c-4271-9296-9e621d536ce1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtR-23CPVMfa"
   },
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "Once you have successfully trained a network you can tune your hyparameters to increase your accuracy.\n",
    "\n",
    "Based on the graphs of the loss function above you should be able to develop some intuition about what hyperparameter adjustments may be necessary. A very noisy loss implies that the learning rate might be too high, while a linearly decreasing loss would suggest that the learning rate may be too low. A large gap between training and validation accuracy would suggest overfitting due to large model without much regularization. No gap between training and validation accuracy would indicate low model capacity (low model complexity). \n",
    "\n",
    "\n",
    "You will compare networks of two and three layers using the different activation functions you implemented. \n",
    "\n",
    "The different hyperparameters you can experiment with are:\n",
    "- **Batch size**: We recommend you leave this at 200 initially which is the batch size we used. \n",
    "- **Number of iterations**: You can gain an intuition for how many iterations to run by checking when the validation accuracy plateaus in your train/val accuracy graph.\n",
    "- **Initialization** Weight initialization is very important for neural networks. We used the initialization `W = np.random.randn(n) / sqrt(n)` where `n` is the input dimension for layer corresponding to `W`. We recommend you stick with the given initializations, but you may explore modifying these. Typical initialization practices: http://cs231n.github.io/neural-networks-2/#init\n",
    "- **Learning rate**: Generally from around 1e-4 to 1e-1 is a good range to explore according to our implementation.\n",
    "- **Learning rate decay**: We recommend a 0.95 decay to start.\n",
    "- **Hidden layer size**: You should explore up to around 120 units per layer. For three-layer network, we fixed the two hidden layers to be the same size when obtaining the target numbers. However, you may experiment with having different size hidden layers.\n",
    "- **Regularization coefficient**: We recommend trying values in the range 0 to 0.1. \n",
    "\n",
    "\n",
    "\n",
    "Hints:\n",
    "- After getting a sense of the parameters by trying a few values yourself, you will likely want to write a few for loops to traverse over a set of hyperparameters.\n",
    "- If you find that your train loss is decreasing, but your train and val accuracy start to decrease rather than increase, your model likely started minimizing the regularization term. To prevent this you will need to decrease the regularization coefficient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izhzVeUJ7nX5"
   },
   "source": [
    "## DEFINING MATRIX OF PARAMETERS (ADDED BY OR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1601351382179,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "A00tXcH67ep_"
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#                     DEFINING THE MATRIX OF PARAMETERS                         #\n",
    "#        THIS MATRIX WILL BE USED FOR OPTIMIZATION OF ALL THE MODELS            #\n",
    "#################################################################################\n",
    "\n",
    "learning_rate_values = [0.1, 0.05, 0.03]\n",
    "hidden_size_values = [20, 50, 80, 100, 120]\n",
    "batch_size_values = [50, 100, 200]\n",
    "\n",
    "#All of this is to build the parameter matrix\n",
    "\n",
    "#Initializing the parameters matrix\n",
    "\n",
    "parameters_matrix = np.zeros((len(learning_rate_values)*len(hidden_size_values)*len(batch_size_values),3))\n",
    "\n",
    "#Entering the learning rate values\n",
    "\n",
    "parameters_matrix[0:len(hidden_size_values)*len(batch_size_values)][:,0] = learning_rate_values[0]\n",
    "parameters_matrix[len(hidden_size_values)*len(batch_size_values):2*len(hidden_size_values)*len(batch_size_values)][:,0] = learning_rate_values[1]\n",
    "parameters_matrix[2*len(hidden_size_values)*len(batch_size_values):3*len(hidden_size_values)*len(batch_size_values)][:,0] = learning_rate_values[2]\n",
    "\n",
    "#Entering the hidden size values\n",
    "\n",
    "parameters_matrix[0:len(parameters_matrix):5,1] = hidden_size_values[0]\n",
    "parameters_matrix[1:len(parameters_matrix):5,1] = hidden_size_values[1]\n",
    "parameters_matrix[2:len(parameters_matrix):5,1] = hidden_size_values[2]\n",
    "parameters_matrix[3:len(parameters_matrix):5,1] = hidden_size_values[3]\n",
    "parameters_matrix[4:len(parameters_matrix):5,1] = hidden_size_values[4]\n",
    "\n",
    "#Entering the hidden size values\n",
    "\n",
    "parameters_matrix[0:len(parameters_matrix):3,2] = batch_size_values[0]\n",
    "parameters_matrix[1:len(parameters_matrix):3,2] = batch_size_values[1]\n",
    "parameters_matrix[2:len(parameters_matrix):3,2] = batch_size_values[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Adg4z5sEVMfa"
   },
   "source": [
    "## Two-layer Relu Activation Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 435483,
     "status": "ok",
     "timestamp": 1601351825740,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "DmMfLeqBVMfb",
    "outputId": "42c431a2-d384-4388-80c2-29600bd73287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 2.715286\n",
      "iteration 100 / 1000: loss 1.866864\n",
      "iteration 200 / 1000: loss 1.913203\n",
      "iteration 300 / 1000: loss 1.814135\n",
      "iteration 400 / 1000: loss 1.981856\n",
      "iteration 500 / 1000: loss 1.978752\n",
      "iteration 600 / 1000: loss 1.811956\n",
      "iteration 700 / 1000: loss 1.678494\n",
      "iteration 800 / 1000: loss 1.764641\n",
      "iteration 900 / 1000: loss 1.839433\n",
      "For parameters:  [ 0.1 20.  50. ]\n",
      "Validation accuracy:  0.398\n",
      "iteration 0 / 1000: loss 2.542021\n",
      "iteration 100 / 1000: loss 1.683022\n",
      "iteration 200 / 1000: loss 1.664625\n",
      "iteration 300 / 1000: loss 1.557218\n",
      "iteration 400 / 1000: loss 1.844410\n",
      "iteration 500 / 1000: loss 1.750250\n",
      "iteration 600 / 1000: loss 1.544880\n",
      "iteration 700 / 1000: loss 1.627553\n",
      "iteration 800 / 1000: loss 1.524133\n",
      "iteration 900 / 1000: loss 1.440771\n",
      "For parameters:  [  0.1  50.  100. ]\n",
      "Validation accuracy:  0.473\n",
      "iteration 0 / 1000: loss 2.428241\n",
      "iteration 100 / 1000: loss 1.796744\n",
      "iteration 200 / 1000: loss 1.546376\n",
      "iteration 300 / 1000: loss 1.577306\n",
      "iteration 400 / 1000: loss 1.571245\n",
      "iteration 500 / 1000: loss 1.298537\n",
      "iteration 600 / 1000: loss 1.566067\n",
      "iteration 700 / 1000: loss 1.336600\n",
      "iteration 800 / 1000: loss 1.361602\n",
      "iteration 900 / 1000: loss 1.421368\n",
      "For parameters:  [1.e-01 8.e+01 2.e+02]\n",
      "Validation accuracy:  0.489\n",
      "iteration 0 / 1000: loss 2.434387\n",
      "iteration 100 / 1000: loss 1.897614\n",
      "iteration 200 / 1000: loss 1.790517\n",
      "iteration 300 / 1000: loss 1.549533\n",
      "iteration 400 / 1000: loss 1.756254\n",
      "iteration 500 / 1000: loss 1.785943\n",
      "iteration 600 / 1000: loss 1.959229\n",
      "iteration 700 / 1000: loss 1.622413\n",
      "iteration 800 / 1000: loss 1.623365\n",
      "iteration 900 / 1000: loss 2.029529\n",
      "For parameters:  [  0.1 100.   50. ]\n",
      "Validation accuracy:  0.414\n",
      "iteration 0 / 1000: loss 2.376951\n",
      "iteration 100 / 1000: loss 1.788000\n",
      "iteration 200 / 1000: loss 1.559161\n",
      "iteration 300 / 1000: loss 1.560658\n",
      "iteration 400 / 1000: loss 1.558631\n",
      "iteration 500 / 1000: loss 1.715426\n",
      "iteration 600 / 1000: loss 1.543630\n",
      "iteration 700 / 1000: loss 1.522090\n",
      "iteration 800 / 1000: loss 1.422984\n",
      "iteration 900 / 1000: loss 1.484674\n",
      "For parameters:  [1.0e-01 1.2e+02 1.0e+02]\n",
      "Validation accuracy:  0.468\n",
      "iteration 0 / 1000: loss 2.426119\n",
      "iteration 100 / 1000: loss 1.717044\n",
      "iteration 200 / 1000: loss 1.629928\n",
      "iteration 300 / 1000: loss 1.697360\n",
      "iteration 400 / 1000: loss 1.646700\n",
      "iteration 500 / 1000: loss 1.741680\n",
      "iteration 600 / 1000: loss 1.713300\n",
      "iteration 700 / 1000: loss 1.547610\n",
      "iteration 800 / 1000: loss 1.525743\n",
      "iteration 900 / 1000: loss 1.465556\n",
      "For parameters:  [1.e-01 2.e+01 2.e+02]\n",
      "Validation accuracy:  0.441\n",
      "iteration 0 / 1000: loss 2.662212\n",
      "iteration 100 / 1000: loss 1.804719\n",
      "iteration 200 / 1000: loss 1.825623\n",
      "iteration 300 / 1000: loss 1.827037\n",
      "iteration 400 / 1000: loss 1.991980\n",
      "iteration 500 / 1000: loss 1.731884\n",
      "iteration 600 / 1000: loss 1.457862\n",
      "iteration 700 / 1000: loss 1.559146\n",
      "iteration 800 / 1000: loss 1.554897\n",
      "iteration 900 / 1000: loss 1.445868\n",
      "For parameters:  [ 0.1 50.  50. ]\n",
      "Validation accuracy:  0.36\n",
      "iteration 0 / 1000: loss 2.518125\n",
      "iteration 100 / 1000: loss 1.738500\n",
      "iteration 200 / 1000: loss 1.639498\n",
      "iteration 300 / 1000: loss 1.994568\n",
      "iteration 400 / 1000: loss 1.681102\n",
      "iteration 500 / 1000: loss 1.672262\n",
      "iteration 600 / 1000: loss 1.432016\n",
      "iteration 700 / 1000: loss 1.425032\n",
      "iteration 800 / 1000: loss 1.553365\n",
      "iteration 900 / 1000: loss 1.344180\n",
      "For parameters:  [  0.1  80.  100. ]\n",
      "Validation accuracy:  0.418\n",
      "iteration 0 / 1000: loss 2.380153\n",
      "iteration 100 / 1000: loss 1.728520\n",
      "iteration 200 / 1000: loss 1.534835\n",
      "iteration 300 / 1000: loss 1.449737\n",
      "iteration 400 / 1000: loss 1.362949\n",
      "iteration 500 / 1000: loss 1.485556\n",
      "iteration 600 / 1000: loss 1.462125\n",
      "iteration 700 / 1000: loss 1.338245\n",
      "iteration 800 / 1000: loss 1.258395\n",
      "iteration 900 / 1000: loss 1.283329\n",
      "For parameters:  [1.e-01 1.e+02 2.e+02]\n",
      "Validation accuracy:  0.5\n",
      "iteration 0 / 1000: loss 2.525483\n",
      "iteration 100 / 1000: loss 1.878190\n",
      "iteration 200 / 1000: loss 1.749712\n",
      "iteration 300 / 1000: loss 1.666091\n",
      "iteration 400 / 1000: loss 1.970726\n",
      "iteration 500 / 1000: loss 2.062984\n",
      "iteration 600 / 1000: loss 1.614487\n",
      "iteration 700 / 1000: loss 1.638533\n",
      "iteration 800 / 1000: loss 1.766345\n",
      "iteration 900 / 1000: loss 1.767084\n",
      "For parameters:  [1.0e-01 1.2e+02 5.0e+01]\n",
      "Validation accuracy:  0.418\n",
      "iteration 0 / 1000: loss 2.606611\n",
      "iteration 100 / 1000: loss 1.730961\n",
      "iteration 200 / 1000: loss 1.743571\n",
      "iteration 300 / 1000: loss 1.783963\n",
      "iteration 400 / 1000: loss 1.847133\n",
      "iteration 500 / 1000: loss 1.416942\n",
      "iteration 600 / 1000: loss 1.683316\n",
      "iteration 700 / 1000: loss 1.667880\n",
      "iteration 800 / 1000: loss 1.747269\n",
      "iteration 900 / 1000: loss 1.620577\n",
      "For parameters:  [  0.1  20.  100. ]\n",
      "Validation accuracy:  0.43\n",
      "iteration 0 / 1000: loss 2.498759\n",
      "iteration 100 / 1000: loss 1.711566\n",
      "iteration 200 / 1000: loss 1.572389\n",
      "iteration 300 / 1000: loss 1.591275\n",
      "iteration 400 / 1000: loss 1.555117\n",
      "iteration 500 / 1000: loss 1.541600\n",
      "iteration 600 / 1000: loss 1.518847\n",
      "iteration 700 / 1000: loss 1.381872\n",
      "iteration 800 / 1000: loss 1.422076\n",
      "iteration 900 / 1000: loss 1.315779\n",
      "For parameters:  [1.e-01 5.e+01 2.e+02]\n",
      "Validation accuracy:  0.477\n",
      "iteration 0 / 1000: loss 2.480604\n",
      "iteration 100 / 1000: loss 1.962881\n",
      "iteration 200 / 1000: loss 1.836400\n",
      "iteration 300 / 1000: loss 1.554923\n",
      "iteration 400 / 1000: loss 1.772844\n",
      "iteration 500 / 1000: loss 1.915968\n",
      "iteration 600 / 1000: loss 1.679191\n",
      "iteration 700 / 1000: loss 1.792873\n",
      "iteration 800 / 1000: loss 1.380780\n",
      "iteration 900 / 1000: loss 1.774523\n",
      "For parameters:  [ 0.1 80.  50. ]\n",
      "Validation accuracy:  0.44\n",
      "iteration 0 / 1000: loss 2.475962\n",
      "iteration 100 / 1000: loss 1.697423\n",
      "iteration 200 / 1000: loss 1.684923\n",
      "iteration 300 / 1000: loss 1.464603\n",
      "iteration 400 / 1000: loss 1.566533\n",
      "iteration 500 / 1000: loss 1.393598\n",
      "iteration 600 / 1000: loss 1.483803\n",
      "iteration 700 / 1000: loss 1.518905\n",
      "iteration 800 / 1000: loss 1.502741\n",
      "iteration 900 / 1000: loss 1.444068\n",
      "For parameters:  [  0.1 100.  100. ]\n",
      "Validation accuracy:  0.466\n",
      "iteration 0 / 1000: loss 2.507818\n",
      "iteration 100 / 1000: loss 1.661972\n",
      "iteration 200 / 1000: loss 1.693131\n",
      "iteration 300 / 1000: loss 1.489109\n",
      "iteration 400 / 1000: loss 1.571926\n",
      "iteration 500 / 1000: loss 1.349370\n",
      "iteration 600 / 1000: loss 1.234844\n",
      "iteration 700 / 1000: loss 1.226016\n",
      "iteration 800 / 1000: loss 1.545961\n",
      "iteration 900 / 1000: loss 1.250473\n",
      "For parameters:  [1.0e-01 1.2e+02 2.0e+02]\n",
      "Validation accuracy:  0.524\n",
      "iteration 0 / 1000: loss 2.649751\n",
      "iteration 100 / 1000: loss 2.082025\n",
      "iteration 200 / 1000: loss 1.839678\n",
      "iteration 300 / 1000: loss 1.873448\n",
      "iteration 400 / 1000: loss 1.777536\n",
      "iteration 500 / 1000: loss 1.778407\n",
      "iteration 600 / 1000: loss 1.748325\n",
      "iteration 700 / 1000: loss 1.654586\n",
      "iteration 800 / 1000: loss 1.835584\n",
      "iteration 900 / 1000: loss 1.731624\n",
      "For parameters:  [ 0.05 20.   50.  ]\n",
      "Validation accuracy:  0.399\n",
      "iteration 0 / 1000: loss 2.438073\n",
      "iteration 100 / 1000: loss 1.774453\n",
      "iteration 200 / 1000: loss 1.587868\n",
      "iteration 300 / 1000: loss 1.613218\n",
      "iteration 400 / 1000: loss 1.685128\n",
      "iteration 500 / 1000: loss 1.481976\n",
      "iteration 600 / 1000: loss 1.693441\n",
      "iteration 700 / 1000: loss 1.522465\n",
      "iteration 800 / 1000: loss 1.260546\n",
      "iteration 900 / 1000: loss 1.456370\n",
      "For parameters:  [5.e-02 5.e+01 1.e+02]\n",
      "Validation accuracy:  0.473\n",
      "iteration 0 / 1000: loss 2.609454\n",
      "iteration 100 / 1000: loss 1.655263\n",
      "iteration 200 / 1000: loss 1.566539\n",
      "iteration 300 / 1000: loss 1.617915\n",
      "iteration 400 / 1000: loss 1.508518\n",
      "iteration 500 / 1000: loss 1.480823\n",
      "iteration 600 / 1000: loss 1.466878\n",
      "iteration 700 / 1000: loss 1.452852\n",
      "iteration 800 / 1000: loss 1.281365\n",
      "iteration 900 / 1000: loss 1.332153\n",
      "For parameters:  [5.e-02 8.e+01 2.e+02]\n",
      "Validation accuracy:  0.498\n",
      "iteration 0 / 1000: loss 2.340921\n",
      "iteration 100 / 1000: loss 1.618954\n",
      "iteration 200 / 1000: loss 1.785193\n",
      "iteration 300 / 1000: loss 1.815763\n",
      "iteration 400 / 1000: loss 1.767731\n",
      "iteration 500 / 1000: loss 1.689865\n",
      "iteration 600 / 1000: loss 1.502381\n",
      "iteration 700 / 1000: loss 1.792459\n",
      "iteration 800 / 1000: loss 1.373051\n",
      "iteration 900 / 1000: loss 1.661950\n",
      "For parameters:  [5.e-02 1.e+02 5.e+01]\n",
      "Validation accuracy:  0.458\n",
      "iteration 0 / 1000: loss 2.420423\n",
      "iteration 100 / 1000: loss 1.847190\n",
      "iteration 200 / 1000: loss 1.671877\n",
      "iteration 300 / 1000: loss 1.777128\n",
      "iteration 400 / 1000: loss 1.563582\n",
      "iteration 500 / 1000: loss 1.404980\n",
      "iteration 600 / 1000: loss 1.405071\n",
      "iteration 700 / 1000: loss 1.259767\n",
      "iteration 800 / 1000: loss 1.368514\n",
      "iteration 900 / 1000: loss 1.461776\n",
      "For parameters:  [5.0e-02 1.2e+02 1.0e+02]\n",
      "Validation accuracy:  0.483\n",
      "iteration 0 / 1000: loss 2.461378\n",
      "iteration 100 / 1000: loss 1.787870\n",
      "iteration 200 / 1000: loss 1.709388\n",
      "iteration 300 / 1000: loss 1.716957\n",
      "iteration 400 / 1000: loss 1.573373\n",
      "iteration 500 / 1000: loss 1.588788\n",
      "iteration 600 / 1000: loss 1.542064\n",
      "iteration 700 / 1000: loss 1.532098\n",
      "iteration 800 / 1000: loss 1.496796\n",
      "iteration 900 / 1000: loss 1.462122\n",
      "For parameters:  [5.e-02 2.e+01 2.e+02]\n",
      "Validation accuracy:  0.445\n",
      "iteration 0 / 1000: loss 2.407305\n",
      "iteration 100 / 1000: loss 1.643106\n",
      "iteration 200 / 1000: loss 2.024314\n",
      "iteration 300 / 1000: loss 1.613249\n",
      "iteration 400 / 1000: loss 1.446963\n",
      "iteration 500 / 1000: loss 1.643224\n",
      "iteration 600 / 1000: loss 1.396599\n",
      "iteration 700 / 1000: loss 1.382745\n",
      "iteration 800 / 1000: loss 1.682422\n",
      "iteration 900 / 1000: loss 1.584512\n",
      "For parameters:  [ 0.05 50.   50.  ]\n",
      "Validation accuracy:  0.443\n",
      "iteration 0 / 1000: loss 2.412643\n",
      "iteration 100 / 1000: loss 1.810660\n",
      "iteration 200 / 1000: loss 1.513533\n",
      "iteration 300 / 1000: loss 1.638817\n",
      "iteration 400 / 1000: loss 1.532174\n",
      "iteration 500 / 1000: loss 1.508489\n",
      "iteration 600 / 1000: loss 1.628086\n",
      "iteration 700 / 1000: loss 1.277978\n",
      "iteration 800 / 1000: loss 1.360869\n",
      "iteration 900 / 1000: loss 1.420019\n",
      "For parameters:  [5.e-02 8.e+01 1.e+02]\n",
      "Validation accuracy:  0.469\n",
      "iteration 0 / 1000: loss 2.624492\n",
      "iteration 100 / 1000: loss 1.628949\n",
      "iteration 200 / 1000: loss 1.541047\n",
      "iteration 300 / 1000: loss 1.472769\n",
      "iteration 400 / 1000: loss 1.457646\n",
      "iteration 500 / 1000: loss 1.513345\n",
      "iteration 600 / 1000: loss 1.428303\n",
      "iteration 700 / 1000: loss 1.321445\n",
      "iteration 800 / 1000: loss 1.296052\n",
      "iteration 900 / 1000: loss 1.409009\n",
      "For parameters:  [5.e-02 1.e+02 2.e+02]\n",
      "Validation accuracy:  0.5\n",
      "iteration 0 / 1000: loss 2.735369\n",
      "iteration 100 / 1000: loss 1.821706\n",
      "iteration 200 / 1000: loss 1.535711\n",
      "iteration 300 / 1000: loss 1.557933\n",
      "iteration 400 / 1000: loss 1.609216\n",
      "iteration 500 / 1000: loss 1.458676\n",
      "iteration 600 / 1000: loss 1.513089\n",
      "iteration 700 / 1000: loss 1.433682\n",
      "iteration 800 / 1000: loss 1.759217\n",
      "iteration 900 / 1000: loss 1.533537\n",
      "For parameters:  [5.0e-02 1.2e+02 5.0e+01]\n",
      "Validation accuracy:  0.448\n",
      "iteration 0 / 1000: loss 2.558711\n",
      "iteration 100 / 1000: loss 1.891120\n",
      "iteration 200 / 1000: loss 1.714737\n",
      "iteration 300 / 1000: loss 1.686663\n",
      "iteration 400 / 1000: loss 1.608266\n",
      "iteration 500 / 1000: loss 1.538979\n",
      "iteration 600 / 1000: loss 1.469570\n",
      "iteration 700 / 1000: loss 1.824436\n",
      "iteration 800 / 1000: loss 1.551256\n",
      "iteration 900 / 1000: loss 1.894040\n",
      "For parameters:  [5.e-02 2.e+01 1.e+02]\n",
      "Validation accuracy:  0.43\n",
      "iteration 0 / 1000: loss 2.450733\n",
      "iteration 100 / 1000: loss 1.867561\n",
      "iteration 200 / 1000: loss 1.568302\n",
      "iteration 300 / 1000: loss 1.579026\n",
      "iteration 400 / 1000: loss 1.488592\n",
      "iteration 500 / 1000: loss 1.438244\n",
      "iteration 600 / 1000: loss 1.514497\n",
      "iteration 700 / 1000: loss 1.348599\n",
      "iteration 800 / 1000: loss 1.471637\n",
      "iteration 900 / 1000: loss 1.403929\n",
      "For parameters:  [5.e-02 5.e+01 2.e+02]\n",
      "Validation accuracy:  0.483\n",
      "iteration 0 / 1000: loss 2.804274\n",
      "iteration 100 / 1000: loss 1.446387\n",
      "iteration 200 / 1000: loss 1.714435\n",
      "iteration 300 / 1000: loss 1.557163\n",
      "iteration 400 / 1000: loss 1.360716\n",
      "iteration 500 / 1000: loss 1.639995\n",
      "iteration 600 / 1000: loss 1.620307\n",
      "iteration 700 / 1000: loss 1.712601\n",
      "iteration 800 / 1000: loss 1.598090\n",
      "iteration 900 / 1000: loss 1.427387\n",
      "For parameters:  [5.e-02 8.e+01 5.e+01]\n",
      "Validation accuracy:  0.451\n",
      "iteration 0 / 1000: loss 2.474303\n",
      "iteration 100 / 1000: loss 1.678994\n",
      "iteration 200 / 1000: loss 1.629488\n",
      "iteration 300 / 1000: loss 1.396176\n",
      "iteration 400 / 1000: loss 1.691628\n",
      "iteration 500 / 1000: loss 1.627583\n",
      "iteration 600 / 1000: loss 1.309060\n",
      "iteration 700 / 1000: loss 1.322883\n",
      "iteration 800 / 1000: loss 1.460409\n",
      "iteration 900 / 1000: loss 1.629806\n",
      "For parameters:  [5.e-02 1.e+02 1.e+02]\n",
      "Validation accuracy:  0.472\n",
      "iteration 0 / 1000: loss 2.661310\n",
      "iteration 100 / 1000: loss 1.739933\n",
      "iteration 200 / 1000: loss 1.568916\n",
      "iteration 300 / 1000: loss 1.407349\n",
      "iteration 400 / 1000: loss 1.550511\n",
      "iteration 500 / 1000: loss 1.470718\n",
      "iteration 600 / 1000: loss 1.301667\n",
      "iteration 700 / 1000: loss 1.432915\n",
      "iteration 800 / 1000: loss 1.474231\n",
      "iteration 900 / 1000: loss 1.419195\n",
      "For parameters:  [5.0e-02 1.2e+02 2.0e+02]\n",
      "Validation accuracy:  0.507\n",
      "iteration 0 / 1000: loss 2.478321\n",
      "iteration 100 / 1000: loss 1.991663\n",
      "iteration 200 / 1000: loss 1.918174\n",
      "iteration 300 / 1000: loss 1.721801\n",
      "iteration 400 / 1000: loss 1.792469\n",
      "iteration 500 / 1000: loss 1.754125\n",
      "iteration 600 / 1000: loss 2.038703\n",
      "iteration 700 / 1000: loss 1.731625\n",
      "iteration 800 / 1000: loss 1.648066\n",
      "iteration 900 / 1000: loss 1.600500\n",
      "For parameters:  [3.e-02 2.e+01 5.e+01]\n",
      "Validation accuracy:  0.411\n",
      "iteration 0 / 1000: loss 2.555676\n",
      "iteration 100 / 1000: loss 1.851040\n",
      "iteration 200 / 1000: loss 1.783851\n",
      "iteration 300 / 1000: loss 1.622707\n",
      "iteration 400 / 1000: loss 1.575700\n",
      "iteration 500 / 1000: loss 1.636633\n",
      "iteration 600 / 1000: loss 1.540888\n",
      "iteration 700 / 1000: loss 1.532103\n",
      "iteration 800 / 1000: loss 1.642933\n",
      "iteration 900 / 1000: loss 1.487687\n",
      "For parameters:  [3.e-02 5.e+01 1.e+02]\n",
      "Validation accuracy:  0.479\n",
      "iteration 0 / 1000: loss 2.516289\n",
      "iteration 100 / 1000: loss 1.631177\n",
      "iteration 200 / 1000: loss 1.585127\n",
      "iteration 300 / 1000: loss 1.475524\n",
      "iteration 400 / 1000: loss 1.555140\n",
      "iteration 500 / 1000: loss 1.549211\n",
      "iteration 600 / 1000: loss 1.479004\n",
      "iteration 700 / 1000: loss 1.484843\n",
      "iteration 800 / 1000: loss 1.455360\n",
      "iteration 900 / 1000: loss 1.289475\n",
      "For parameters:  [3.e-02 8.e+01 2.e+02]\n",
      "Validation accuracy:  0.491\n",
      "iteration 0 / 1000: loss 2.526589\n",
      "iteration 100 / 1000: loss 1.678577\n",
      "iteration 200 / 1000: loss 1.616626\n",
      "iteration 300 / 1000: loss 1.706297\n",
      "iteration 400 / 1000: loss 1.816006\n",
      "iteration 500 / 1000: loss 1.996626\n",
      "iteration 600 / 1000: loss 1.174053\n",
      "iteration 700 / 1000: loss 1.475161\n",
      "iteration 800 / 1000: loss 1.351806\n",
      "iteration 900 / 1000: loss 1.625959\n",
      "For parameters:  [3.e-02 1.e+02 5.e+01]\n",
      "Validation accuracy:  0.464\n",
      "iteration 0 / 1000: loss 2.451267\n",
      "iteration 100 / 1000: loss 1.872902\n",
      "iteration 200 / 1000: loss 1.718976\n",
      "iteration 300 / 1000: loss 1.510867\n",
      "iteration 400 / 1000: loss 1.567572\n",
      "iteration 500 / 1000: loss 1.536055\n",
      "iteration 600 / 1000: loss 1.454384\n",
      "iteration 700 / 1000: loss 1.620166\n",
      "iteration 800 / 1000: loss 1.520594\n",
      "iteration 900 / 1000: loss 1.520137\n",
      "For parameters:  [3.0e-02 1.2e+02 1.0e+02]\n",
      "Validation accuracy:  0.475\n",
      "iteration 0 / 1000: loss 2.491889\n",
      "iteration 100 / 1000: loss 1.755961\n",
      "iteration 200 / 1000: loss 1.769272\n",
      "iteration 300 / 1000: loss 1.588391\n",
      "iteration 400 / 1000: loss 1.688738\n",
      "iteration 500 / 1000: loss 1.702200\n",
      "iteration 600 / 1000: loss 1.547895\n",
      "iteration 700 / 1000: loss 1.578232\n",
      "iteration 800 / 1000: loss 1.568111\n",
      "iteration 900 / 1000: loss 1.724036\n",
      "For parameters:  [3.e-02 2.e+01 2.e+02]\n",
      "Validation accuracy:  0.416\n",
      "iteration 0 / 1000: loss 2.665119\n",
      "iteration 100 / 1000: loss 2.003268\n",
      "iteration 200 / 1000: loss 1.749592\n",
      "iteration 300 / 1000: loss 1.840942\n",
      "iteration 400 / 1000: loss 2.000071\n",
      "iteration 500 / 1000: loss 1.545833\n",
      "iteration 600 / 1000: loss 1.693547\n",
      "iteration 700 / 1000: loss 1.438741\n",
      "iteration 800 / 1000: loss 1.539980\n",
      "iteration 900 / 1000: loss 1.552777\n",
      "For parameters:  [3.e-02 5.e+01 5.e+01]\n",
      "Validation accuracy:  0.433\n",
      "iteration 0 / 1000: loss 2.631301\n",
      "iteration 100 / 1000: loss 1.838058\n",
      "iteration 200 / 1000: loss 1.506151\n",
      "iteration 300 / 1000: loss 1.747571\n",
      "iteration 400 / 1000: loss 1.568485\n",
      "iteration 500 / 1000: loss 1.609352\n",
      "iteration 600 / 1000: loss 1.480039\n",
      "iteration 700 / 1000: loss 1.310527\n",
      "iteration 800 / 1000: loss 1.458614\n",
      "iteration 900 / 1000: loss 1.369625\n",
      "For parameters:  [3.e-02 8.e+01 1.e+02]\n",
      "Validation accuracy:  0.472\n",
      "iteration 0 / 1000: loss 2.661046\n",
      "iteration 100 / 1000: loss 1.756782\n",
      "iteration 200 / 1000: loss 1.621254\n",
      "iteration 300 / 1000: loss 1.510133\n",
      "iteration 400 / 1000: loss 1.586507\n",
      "iteration 500 / 1000: loss 1.560481\n",
      "iteration 600 / 1000: loss 1.540028\n",
      "iteration 700 / 1000: loss 1.459255\n",
      "iteration 800 / 1000: loss 1.304648\n",
      "iteration 900 / 1000: loss 1.386875\n",
      "For parameters:  [3.e-02 1.e+02 2.e+02]\n",
      "Validation accuracy:  0.491\n",
      "iteration 0 / 1000: loss 2.574094\n",
      "iteration 100 / 1000: loss 1.983784\n",
      "iteration 200 / 1000: loss 1.645728\n",
      "iteration 300 / 1000: loss 1.448059\n",
      "iteration 400 / 1000: loss 1.729623\n",
      "iteration 500 / 1000: loss 1.657289\n",
      "iteration 600 / 1000: loss 1.760682\n",
      "iteration 700 / 1000: loss 1.624786\n",
      "iteration 800 / 1000: loss 1.543331\n",
      "iteration 900 / 1000: loss 1.804036\n",
      "For parameters:  [3.0e-02 1.2e+02 5.0e+01]\n",
      "Validation accuracy:  0.451\n",
      "iteration 0 / 1000: loss 2.360071\n",
      "iteration 100 / 1000: loss 2.012974\n",
      "iteration 200 / 1000: loss 1.779618\n",
      "iteration 300 / 1000: loss 1.784505\n",
      "iteration 400 / 1000: loss 1.615778\n",
      "iteration 500 / 1000: loss 1.592882\n",
      "iteration 600 / 1000: loss 1.653272\n",
      "iteration 700 / 1000: loss 1.398708\n",
      "iteration 800 / 1000: loss 1.820599\n",
      "iteration 900 / 1000: loss 1.541349\n",
      "For parameters:  [3.e-02 2.e+01 1.e+02]\n",
      "Validation accuracy:  0.426\n",
      "iteration 0 / 1000: loss 2.670196\n",
      "iteration 100 / 1000: loss 1.696313\n",
      "iteration 200 / 1000: loss 1.694602\n",
      "iteration 300 / 1000: loss 1.589026\n",
      "iteration 400 / 1000: loss 1.552249\n",
      "iteration 500 / 1000: loss 1.499694\n",
      "iteration 600 / 1000: loss 1.536549\n",
      "iteration 700 / 1000: loss 1.542548\n",
      "iteration 800 / 1000: loss 1.389237\n",
      "iteration 900 / 1000: loss 1.401183\n",
      "For parameters:  [3.e-02 5.e+01 2.e+02]\n",
      "Validation accuracy:  0.485\n",
      "iteration 0 / 1000: loss 2.539125\n",
      "iteration 100 / 1000: loss 2.057166\n",
      "iteration 200 / 1000: loss 1.845248\n",
      "iteration 300 / 1000: loss 1.528448\n",
      "iteration 400 / 1000: loss 1.707525\n",
      "iteration 500 / 1000: loss 1.816928\n",
      "iteration 600 / 1000: loss 1.528792\n",
      "iteration 700 / 1000: loss 1.604526\n",
      "iteration 800 / 1000: loss 1.640242\n",
      "iteration 900 / 1000: loss 1.621079\n",
      "For parameters:  [3.e-02 8.e+01 5.e+01]\n",
      "Validation accuracy:  0.455\n",
      "iteration 0 / 1000: loss 2.432269\n",
      "iteration 100 / 1000: loss 1.923649\n",
      "iteration 200 / 1000: loss 1.557253\n",
      "iteration 300 / 1000: loss 1.481281\n",
      "iteration 400 / 1000: loss 1.744673\n",
      "iteration 500 / 1000: loss 1.654912\n",
      "iteration 600 / 1000: loss 1.789358\n",
      "iteration 700 / 1000: loss 1.453594\n",
      "iteration 800 / 1000: loss 1.354064\n",
      "iteration 900 / 1000: loss 1.573829\n",
      "For parameters:  [3.e-02 1.e+02 1.e+02]\n",
      "Validation accuracy:  0.464\n",
      "iteration 0 / 1000: loss 2.622189\n",
      "iteration 100 / 1000: loss 1.698678\n",
      "iteration 200 / 1000: loss 1.612404\n",
      "iteration 300 / 1000: loss 1.483058\n",
      "iteration 400 / 1000: loss 1.544175\n",
      "iteration 500 / 1000: loss 1.466680\n",
      "iteration 600 / 1000: loss 1.372916\n",
      "iteration 700 / 1000: loss 1.443018\n",
      "iteration 800 / 1000: loss 1.327923\n",
      "iteration 900 / 1000: loss 1.355483\n",
      "For parameters:  [3.0e-02 1.2e+02 2.0e+02]\n",
      "Validation accuracy:  0.493\n",
      "The best combination of parameters is: learning rate =  0.1 , hidden layer size =  120 , batch size =  200\n",
      "For this combination, the validation accuracy is  0.524\n"
     ]
    }
   ],
   "source": [
    "best_2layer_relu = None # store the best model into this\n",
    "\n",
    "#################################################################################\n",
    "# TODO: Tune hyperparameters using the validation set. Store your best trained  #\n",
    "# model in best_2layer_relu.                                                    #\n",
    "#################################################################################\n",
    "\n",
    "#Initialize \"best\" validation accuracy\n",
    "\n",
    "best_val_acc = 0\n",
    "\n",
    "#Initialize vector to store all validation accuracies\n",
    "\n",
    "val_acc_vec = np.zeros(len(parameters_matrix))\n",
    "\n",
    "#Cycle through the matrix of parameters\n",
    "\n",
    "for i in range(len(parameters_matrix)):\n",
    "\n",
    "  input_size = 32 * 32 * 3\n",
    "  num_layers = 2\n",
    "  hidden_size = int(parameters_matrix[i,1])\n",
    "  hidden_sizes = [hidden_size]*(num_layers-1)\n",
    "  num_classes = 10\n",
    "  learning_rate = parameters_matrix[i,0]\n",
    "  learning_rate_decay = 0.95\n",
    "  batch_size = int(parameters_matrix[i,2])\n",
    "  net = NeuralNetwork(input_size, hidden_sizes, num_classes, num_layers, nonlinearity='relu')\n",
    "\n",
    "  # Train the network\n",
    "  stats = net.train(X_train, y_train, X_val, y_val,\n",
    "              num_iters=1000, batch_size=batch_size,\n",
    "              learning_rate=learning_rate, learning_rate_decay=learning_rate_decay,\n",
    "              reg=0.0, verbose=True)\n",
    "  \n",
    "  # Predict on the validation set\n",
    "  val_acc = (net.predict(X_val) == y_val).mean()\n",
    "\n",
    "  print('For parameters: ', parameters_matrix[i])\n",
    "  print('Validation accuracy: ', val_acc)\n",
    "\n",
    "  #Store the validaction accuracies\n",
    "\n",
    "  val_acc_vec[i] = val_acc\n",
    "\n",
    "  #Save the best model and stats\n",
    "\n",
    "  if val_acc > best_val_acc:\n",
    "\n",
    "    best_val_acc = val_acc\n",
    "    best_2layer_relu = net\n",
    "    best_stats = stats\n",
    "\n",
    "#Print the best combination of trial parameters:\n",
    "\n",
    "ind = np.argmax(val_acc_vec)\n",
    "\n",
    "print('The best combination of parameters is: learning rate = ',parameters_matrix[ind,0], ', hidden layer size = ',int(parameters_matrix[ind,1]), ', batch size = ',int(parameters_matrix[ind,2]))\n",
    "print('For this combination, the validation accuracy is ', val_acc_vec[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyO7PBTkVMfd"
   },
   "source": [
    "## Two-layer Sigmoid Activation Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 461215,
     "status": "ok",
     "timestamp": 1601352498426,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "0PAIDQvbVMfd",
    "outputId": "0354b414-863d-4733-967b-5335ad7c962a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 2.394454\n",
      "iteration 100 / 1000: loss 1.964292\n",
      "iteration 200 / 1000: loss 1.812530\n",
      "iteration 300 / 1000: loss 1.871928\n",
      "iteration 400 / 1000: loss 1.753414\n",
      "iteration 500 / 1000: loss 2.034388\n",
      "iteration 600 / 1000: loss 1.926047\n",
      "iteration 700 / 1000: loss 1.773440\n",
      "iteration 800 / 1000: loss 1.973519\n",
      "iteration 900 / 1000: loss 1.611529\n",
      "For parameters:  [ 0.1 20.  50. ]\n",
      "Validation accuracy:  0.401\n",
      "iteration 0 / 1000: loss 2.349226\n",
      "iteration 100 / 1000: loss 1.873230\n",
      "iteration 200 / 1000: loss 1.881829\n",
      "iteration 300 / 1000: loss 1.686167\n",
      "iteration 400 / 1000: loss 1.657714\n",
      "iteration 500 / 1000: loss 1.769798\n",
      "iteration 600 / 1000: loss 1.679894\n",
      "iteration 700 / 1000: loss 1.534242\n",
      "iteration 800 / 1000: loss 1.637482\n",
      "iteration 900 / 1000: loss 1.567340\n",
      "For parameters:  [  0.1  50.  100. ]\n",
      "Validation accuracy:  0.42\n",
      "iteration 0 / 1000: loss 2.570874\n",
      "iteration 100 / 1000: loss 1.810058\n",
      "iteration 200 / 1000: loss 1.679592\n",
      "iteration 300 / 1000: loss 1.707853\n",
      "iteration 400 / 1000: loss 1.604382\n",
      "iteration 500 / 1000: loss 1.711257\n",
      "iteration 600 / 1000: loss 1.710577\n",
      "iteration 700 / 1000: loss 1.637946\n",
      "iteration 800 / 1000: loss 1.621874\n",
      "iteration 900 / 1000: loss 1.653794\n",
      "For parameters:  [1.e-01 8.e+01 2.e+02]\n",
      "Validation accuracy:  0.436\n",
      "iteration 0 / 1000: loss 2.465977\n",
      "iteration 100 / 1000: loss 1.940992\n",
      "iteration 200 / 1000: loss 1.906685\n",
      "iteration 300 / 1000: loss 1.810134\n",
      "iteration 400 / 1000: loss 1.861575\n",
      "iteration 500 / 1000: loss 2.028587\n",
      "iteration 600 / 1000: loss 1.566310\n",
      "iteration 700 / 1000: loss 1.476782\n",
      "iteration 800 / 1000: loss 1.578780\n",
      "iteration 900 / 1000: loss 1.679164\n",
      "For parameters:  [  0.1 100.   50. ]\n",
      "Validation accuracy:  0.428\n",
      "iteration 0 / 1000: loss 2.328111\n",
      "iteration 100 / 1000: loss 1.752327\n",
      "iteration 200 / 1000: loss 1.696437\n",
      "iteration 300 / 1000: loss 1.724829\n",
      "iteration 400 / 1000: loss 1.835532\n",
      "iteration 500 / 1000: loss 1.592002\n",
      "iteration 600 / 1000: loss 1.707046\n",
      "iteration 700 / 1000: loss 1.815531\n",
      "iteration 800 / 1000: loss 1.791658\n",
      "iteration 900 / 1000: loss 1.658150\n",
      "For parameters:  [1.0e-01 1.2e+02 1.0e+02]\n",
      "Validation accuracy:  0.416\n",
      "iteration 0 / 1000: loss 2.536106\n",
      "iteration 100 / 1000: loss 1.886215\n",
      "iteration 200 / 1000: loss 1.863925\n",
      "iteration 300 / 1000: loss 1.769263\n",
      "iteration 400 / 1000: loss 1.754592\n",
      "iteration 500 / 1000: loss 1.670776\n",
      "iteration 600 / 1000: loss 1.803482\n",
      "iteration 700 / 1000: loss 1.722791\n",
      "iteration 800 / 1000: loss 1.567201\n",
      "iteration 900 / 1000: loss 1.625860\n",
      "For parameters:  [1.e-01 2.e+01 2.e+02]\n",
      "Validation accuracy:  0.402\n",
      "iteration 0 / 1000: loss 2.420396\n",
      "iteration 100 / 1000: loss 1.794949\n",
      "iteration 200 / 1000: loss 1.744375\n",
      "iteration 300 / 1000: loss 1.741356\n",
      "iteration 400 / 1000: loss 1.634575\n",
      "iteration 500 / 1000: loss 1.591375\n",
      "iteration 600 / 1000: loss 1.441955\n",
      "iteration 700 / 1000: loss 1.763750\n",
      "iteration 800 / 1000: loss 1.647008\n",
      "iteration 900 / 1000: loss 1.649860\n",
      "For parameters:  [ 0.1 50.  50. ]\n",
      "Validation accuracy:  0.403\n",
      "iteration 0 / 1000: loss 2.323143\n",
      "iteration 100 / 1000: loss 1.797190\n",
      "iteration 200 / 1000: loss 1.817047\n",
      "iteration 300 / 1000: loss 1.858731\n",
      "iteration 400 / 1000: loss 1.613066\n",
      "iteration 500 / 1000: loss 1.749125\n",
      "iteration 600 / 1000: loss 1.773327\n",
      "iteration 700 / 1000: loss 1.738443\n",
      "iteration 800 / 1000: loss 1.649997\n",
      "iteration 900 / 1000: loss 1.555992\n",
      "For parameters:  [  0.1  80.  100. ]\n",
      "Validation accuracy:  0.423\n",
      "iteration 0 / 1000: loss 2.488553\n",
      "iteration 100 / 1000: loss 1.757147\n",
      "iteration 200 / 1000: loss 1.788353\n",
      "iteration 300 / 1000: loss 1.831604\n",
      "iteration 400 / 1000: loss 1.739132\n",
      "iteration 500 / 1000: loss 1.644011\n",
      "iteration 600 / 1000: loss 1.656923\n",
      "iteration 700 / 1000: loss 1.537302\n",
      "iteration 800 / 1000: loss 1.562717\n",
      "iteration 900 / 1000: loss 1.671345\n",
      "For parameters:  [1.e-01 1.e+02 2.e+02]\n",
      "Validation accuracy:  0.415\n",
      "iteration 0 / 1000: loss 2.459715\n",
      "iteration 100 / 1000: loss 1.703319\n",
      "iteration 200 / 1000: loss 2.061390\n",
      "iteration 300 / 1000: loss 1.766120\n",
      "iteration 400 / 1000: loss 1.917015\n",
      "iteration 500 / 1000: loss 1.632332\n",
      "iteration 600 / 1000: loss 1.933932\n",
      "iteration 700 / 1000: loss 1.596196\n",
      "iteration 800 / 1000: loss 1.711755\n",
      "iteration 900 / 1000: loss 1.656928\n",
      "For parameters:  [1.0e-01 1.2e+02 5.0e+01]\n",
      "Validation accuracy:  0.413\n",
      "iteration 0 / 1000: loss 2.487347\n",
      "iteration 100 / 1000: loss 1.942118\n",
      "iteration 200 / 1000: loss 1.911423\n",
      "iteration 300 / 1000: loss 1.845727\n",
      "iteration 400 / 1000: loss 1.856512\n",
      "iteration 500 / 1000: loss 1.780575\n",
      "iteration 600 / 1000: loss 1.678178\n",
      "iteration 700 / 1000: loss 1.733185\n",
      "iteration 800 / 1000: loss 1.649580\n",
      "iteration 900 / 1000: loss 1.754960\n",
      "For parameters:  [  0.1  20.  100. ]\n",
      "Validation accuracy:  0.424\n",
      "iteration 0 / 1000: loss 2.406264\n",
      "iteration 100 / 1000: loss 1.820504\n",
      "iteration 200 / 1000: loss 1.799362\n",
      "iteration 300 / 1000: loss 1.744249\n",
      "iteration 400 / 1000: loss 1.750998\n",
      "iteration 500 / 1000: loss 1.575253\n",
      "iteration 600 / 1000: loss 1.722688\n",
      "iteration 700 / 1000: loss 1.661904\n",
      "iteration 800 / 1000: loss 1.515969\n",
      "iteration 900 / 1000: loss 1.555201\n",
      "For parameters:  [1.e-01 5.e+01 2.e+02]\n",
      "Validation accuracy:  0.422\n",
      "iteration 0 / 1000: loss 2.420010\n",
      "iteration 100 / 1000: loss 2.016294\n",
      "iteration 200 / 1000: loss 1.645791\n",
      "iteration 300 / 1000: loss 1.920713\n",
      "iteration 400 / 1000: loss 1.582625\n",
      "iteration 500 / 1000: loss 1.853434\n",
      "iteration 600 / 1000: loss 1.717103\n",
      "iteration 700 / 1000: loss 1.844395\n",
      "iteration 800 / 1000: loss 1.650734\n",
      "iteration 900 / 1000: loss 1.821369\n",
      "For parameters:  [ 0.1 80.  50. ]\n",
      "Validation accuracy:  0.426\n",
      "iteration 0 / 1000: loss 2.487527\n",
      "iteration 100 / 1000: loss 1.918807\n",
      "iteration 200 / 1000: loss 1.560309\n",
      "iteration 300 / 1000: loss 1.634781\n",
      "iteration 400 / 1000: loss 1.819056\n",
      "iteration 500 / 1000: loss 1.778763\n",
      "iteration 600 / 1000: loss 1.759877\n",
      "iteration 700 / 1000: loss 1.539222\n",
      "iteration 800 / 1000: loss 1.625968\n",
      "iteration 900 / 1000: loss 1.633689\n",
      "For parameters:  [  0.1 100.  100. ]\n",
      "Validation accuracy:  0.436\n",
      "iteration 0 / 1000: loss 2.385669\n",
      "iteration 100 / 1000: loss 1.783753\n",
      "iteration 200 / 1000: loss 1.838120\n",
      "iteration 300 / 1000: loss 1.616081\n",
      "iteration 400 / 1000: loss 1.638918\n",
      "iteration 500 / 1000: loss 1.682697\n",
      "iteration 600 / 1000: loss 1.565685\n",
      "iteration 700 / 1000: loss 1.740723\n",
      "iteration 800 / 1000: loss 1.699835\n",
      "iteration 900 / 1000: loss 1.649610\n",
      "For parameters:  [1.0e-01 1.2e+02 2.0e+02]\n",
      "Validation accuracy:  0.424\n",
      "iteration 0 / 1000: loss 2.455716\n",
      "iteration 100 / 1000: loss 2.065960\n",
      "iteration 200 / 1000: loss 1.855749\n",
      "iteration 300 / 1000: loss 2.001173\n",
      "iteration 400 / 1000: loss 1.777433\n",
      "iteration 500 / 1000: loss 1.789087\n",
      "iteration 600 / 1000: loss 1.730583\n",
      "iteration 700 / 1000: loss 1.703707\n",
      "iteration 800 / 1000: loss 1.752186\n",
      "iteration 900 / 1000: loss 1.734850\n",
      "For parameters:  [ 0.05 20.   50.  ]\n",
      "Validation accuracy:  0.408\n",
      "iteration 0 / 1000: loss 2.420198\n",
      "iteration 100 / 1000: loss 1.806616\n",
      "iteration 200 / 1000: loss 1.746601\n",
      "iteration 300 / 1000: loss 1.839837\n",
      "iteration 400 / 1000: loss 1.718981\n",
      "iteration 500 / 1000: loss 1.633449\n",
      "iteration 600 / 1000: loss 1.711333\n",
      "iteration 700 / 1000: loss 1.647152\n",
      "iteration 800 / 1000: loss 1.836232\n",
      "iteration 900 / 1000: loss 1.853690\n",
      "For parameters:  [5.e-02 5.e+01 1.e+02]\n",
      "Validation accuracy:  0.418\n",
      "iteration 0 / 1000: loss 2.373002\n",
      "iteration 100 / 1000: loss 1.904264\n",
      "iteration 200 / 1000: loss 1.760397\n",
      "iteration 300 / 1000: loss 1.734230\n",
      "iteration 400 / 1000: loss 1.643912\n",
      "iteration 500 / 1000: loss 1.779889\n",
      "iteration 600 / 1000: loss 1.701976\n",
      "iteration 700 / 1000: loss 1.780676\n",
      "iteration 800 / 1000: loss 1.657758\n",
      "iteration 900 / 1000: loss 1.732420\n",
      "For parameters:  [5.e-02 8.e+01 2.e+02]\n",
      "Validation accuracy:  0.412\n",
      "iteration 0 / 1000: loss 2.282326\n",
      "iteration 100 / 1000: loss 1.935968\n",
      "iteration 200 / 1000: loss 1.755951\n",
      "iteration 300 / 1000: loss 1.693249\n",
      "iteration 400 / 1000: loss 1.778067\n",
      "iteration 500 / 1000: loss 1.790226\n",
      "iteration 600 / 1000: loss 1.809796\n",
      "iteration 700 / 1000: loss 1.933012\n",
      "iteration 800 / 1000: loss 1.809176\n",
      "iteration 900 / 1000: loss 1.881093\n",
      "For parameters:  [5.e-02 1.e+02 5.e+01]\n",
      "Validation accuracy:  0.423\n",
      "iteration 0 / 1000: loss 2.400095\n",
      "iteration 100 / 1000: loss 1.831414\n",
      "iteration 200 / 1000: loss 1.800341\n",
      "iteration 300 / 1000: loss 1.647824\n",
      "iteration 400 / 1000: loss 1.650170\n",
      "iteration 500 / 1000: loss 1.744460\n",
      "iteration 600 / 1000: loss 1.689547\n",
      "iteration 700 / 1000: loss 1.907491\n",
      "iteration 800 / 1000: loss 1.786880\n",
      "iteration 900 / 1000: loss 1.669094\n",
      "For parameters:  [5.0e-02 1.2e+02 1.0e+02]\n",
      "Validation accuracy:  0.418\n",
      "iteration 0 / 1000: loss 2.476107\n",
      "iteration 100 / 1000: loss 1.960507\n",
      "iteration 200 / 1000: loss 1.858606\n",
      "iteration 300 / 1000: loss 1.862437\n",
      "iteration 400 / 1000: loss 1.893980\n",
      "iteration 500 / 1000: loss 1.803977\n",
      "iteration 600 / 1000: loss 1.747360\n",
      "iteration 700 / 1000: loss 1.792101\n",
      "iteration 800 / 1000: loss 1.754027\n",
      "iteration 900 / 1000: loss 1.756797\n",
      "For parameters:  [5.e-02 2.e+01 2.e+02]\n",
      "Validation accuracy:  0.394\n",
      "iteration 0 / 1000: loss 2.399920\n",
      "iteration 100 / 1000: loss 1.914639\n",
      "iteration 200 / 1000: loss 1.911489\n",
      "iteration 300 / 1000: loss 1.784970\n",
      "iteration 400 / 1000: loss 1.713829\n",
      "iteration 500 / 1000: loss 1.696032\n",
      "iteration 600 / 1000: loss 1.729354\n",
      "iteration 700 / 1000: loss 1.627333\n",
      "iteration 800 / 1000: loss 1.813409\n",
      "iteration 900 / 1000: loss 1.592761\n",
      "For parameters:  [ 0.05 50.   50.  ]\n",
      "Validation accuracy:  0.428\n",
      "iteration 0 / 1000: loss 2.353592\n",
      "iteration 100 / 1000: loss 2.019339\n",
      "iteration 200 / 1000: loss 1.909803\n",
      "iteration 300 / 1000: loss 1.701646\n",
      "iteration 400 / 1000: loss 1.786880\n",
      "iteration 500 / 1000: loss 1.758090\n",
      "iteration 600 / 1000: loss 1.703448\n",
      "iteration 700 / 1000: loss 1.595363\n",
      "iteration 800 / 1000: loss 1.641676\n",
      "iteration 900 / 1000: loss 1.738780\n",
      "For parameters:  [5.e-02 8.e+01 1.e+02]\n",
      "Validation accuracy:  0.42\n",
      "iteration 0 / 1000: loss 2.345795\n",
      "iteration 100 / 1000: loss 1.815835\n",
      "iteration 200 / 1000: loss 1.846279\n",
      "iteration 300 / 1000: loss 1.794705\n",
      "iteration 400 / 1000: loss 1.733719\n",
      "iteration 500 / 1000: loss 1.798558\n",
      "iteration 600 / 1000: loss 1.620289\n",
      "iteration 700 / 1000: loss 1.760604\n",
      "iteration 800 / 1000: loss 1.761495\n",
      "iteration 900 / 1000: loss 1.677075\n",
      "For parameters:  [5.e-02 1.e+02 2.e+02]\n",
      "Validation accuracy:  0.414\n",
      "iteration 0 / 1000: loss 2.469060\n",
      "iteration 100 / 1000: loss 1.930331\n",
      "iteration 200 / 1000: loss 1.770706\n",
      "iteration 300 / 1000: loss 1.733105\n",
      "iteration 400 / 1000: loss 1.754774\n",
      "iteration 500 / 1000: loss 1.604754\n",
      "iteration 600 / 1000: loss 1.831471\n",
      "iteration 700 / 1000: loss 1.644048\n",
      "iteration 800 / 1000: loss 1.546032\n",
      "iteration 900 / 1000: loss 1.649707\n",
      "For parameters:  [5.0e-02 1.2e+02 5.0e+01]\n",
      "Validation accuracy:  0.402\n",
      "iteration 0 / 1000: loss 2.479579\n",
      "iteration 100 / 1000: loss 1.944570\n",
      "iteration 200 / 1000: loss 1.934310\n",
      "iteration 300 / 1000: loss 1.799130\n",
      "iteration 400 / 1000: loss 1.735172\n",
      "iteration 500 / 1000: loss 1.762588\n",
      "iteration 600 / 1000: loss 1.790943\n",
      "iteration 700 / 1000: loss 1.840967\n",
      "iteration 800 / 1000: loss 1.715145\n",
      "iteration 900 / 1000: loss 1.762889\n",
      "For parameters:  [5.e-02 2.e+01 1.e+02]\n",
      "Validation accuracy:  0.41\n",
      "iteration 0 / 1000: loss 2.416241\n",
      "iteration 100 / 1000: loss 1.945121\n",
      "iteration 200 / 1000: loss 1.776174\n",
      "iteration 300 / 1000: loss 1.901165\n",
      "iteration 400 / 1000: loss 1.715005\n",
      "iteration 500 / 1000: loss 1.770175\n",
      "iteration 600 / 1000: loss 1.740622\n",
      "iteration 700 / 1000: loss 1.701591\n",
      "iteration 800 / 1000: loss 1.671792\n",
      "iteration 900 / 1000: loss 1.693164\n",
      "For parameters:  [5.e-02 5.e+01 2.e+02]\n",
      "Validation accuracy:  0.42\n",
      "iteration 0 / 1000: loss 2.375107\n",
      "iteration 100 / 1000: loss 2.043450\n",
      "iteration 200 / 1000: loss 1.928399\n",
      "iteration 300 / 1000: loss 1.785400\n",
      "iteration 400 / 1000: loss 1.687530\n",
      "iteration 500 / 1000: loss 1.859464\n",
      "iteration 600 / 1000: loss 1.617911\n",
      "iteration 700 / 1000: loss 1.607465\n",
      "iteration 800 / 1000: loss 1.714931\n",
      "iteration 900 / 1000: loss 1.572047\n",
      "For parameters:  [5.e-02 8.e+01 5.e+01]\n",
      "Validation accuracy:  0.416\n",
      "iteration 0 / 1000: loss 2.374696\n",
      "iteration 100 / 1000: loss 1.871647\n",
      "iteration 200 / 1000: loss 1.857009\n",
      "iteration 300 / 1000: loss 1.818499\n",
      "iteration 400 / 1000: loss 1.694697\n",
      "iteration 500 / 1000: loss 1.813534\n",
      "iteration 600 / 1000: loss 1.755415\n",
      "iteration 700 / 1000: loss 1.688975\n",
      "iteration 800 / 1000: loss 1.814380\n",
      "iteration 900 / 1000: loss 1.769124\n",
      "For parameters:  [5.e-02 1.e+02 1.e+02]\n",
      "Validation accuracy:  0.406\n",
      "iteration 0 / 1000: loss 2.412279\n",
      "iteration 100 / 1000: loss 1.871401\n",
      "iteration 200 / 1000: loss 1.867153\n",
      "iteration 300 / 1000: loss 1.773904\n",
      "iteration 400 / 1000: loss 1.752046\n",
      "iteration 500 / 1000: loss 1.672935\n",
      "iteration 600 / 1000: loss 1.754616\n",
      "iteration 700 / 1000: loss 1.687278\n",
      "iteration 800 / 1000: loss 1.712759\n",
      "iteration 900 / 1000: loss 1.603706\n",
      "For parameters:  [5.0e-02 1.2e+02 2.0e+02]\n",
      "Validation accuracy:  0.424\n",
      "iteration 0 / 1000: loss 2.350478\n",
      "iteration 100 / 1000: loss 2.026968\n",
      "iteration 200 / 1000: loss 1.971370\n",
      "iteration 300 / 1000: loss 1.808695\n",
      "iteration 400 / 1000: loss 1.902631\n",
      "iteration 500 / 1000: loss 1.901005\n",
      "iteration 600 / 1000: loss 1.766567\n",
      "iteration 700 / 1000: loss 1.691466\n",
      "iteration 800 / 1000: loss 1.998961\n",
      "iteration 900 / 1000: loss 1.698829\n",
      "For parameters:  [3.e-02 2.e+01 5.e+01]\n",
      "Validation accuracy:  0.386\n",
      "iteration 0 / 1000: loss 2.340535\n",
      "iteration 100 / 1000: loss 1.998176\n",
      "iteration 200 / 1000: loss 1.885393\n",
      "iteration 300 / 1000: loss 1.939431\n",
      "iteration 400 / 1000: loss 1.816331\n",
      "iteration 500 / 1000: loss 1.869912\n",
      "iteration 600 / 1000: loss 1.743681\n",
      "iteration 700 / 1000: loss 1.927129\n",
      "iteration 800 / 1000: loss 1.694740\n",
      "iteration 900 / 1000: loss 1.799152\n",
      "For parameters:  [3.e-02 5.e+01 1.e+02]\n",
      "Validation accuracy:  0.399\n",
      "iteration 0 / 1000: loss 2.482252\n",
      "iteration 100 / 1000: loss 1.994098\n",
      "iteration 200 / 1000: loss 1.873706\n",
      "iteration 300 / 1000: loss 1.834685\n",
      "iteration 400 / 1000: loss 1.867152\n",
      "iteration 500 / 1000: loss 1.679867\n",
      "iteration 600 / 1000: loss 1.844308\n",
      "iteration 700 / 1000: loss 1.619347\n",
      "iteration 800 / 1000: loss 1.776429\n",
      "iteration 900 / 1000: loss 1.610273\n",
      "For parameters:  [3.e-02 8.e+01 2.e+02]\n",
      "Validation accuracy:  0.416\n",
      "iteration 0 / 1000: loss 2.486651\n",
      "iteration 100 / 1000: loss 1.908359\n",
      "iteration 200 / 1000: loss 1.848283\n",
      "iteration 300 / 1000: loss 1.599898\n",
      "iteration 400 / 1000: loss 1.836110\n",
      "iteration 500 / 1000: loss 1.751169\n",
      "iteration 600 / 1000: loss 1.821119\n",
      "iteration 700 / 1000: loss 1.906609\n",
      "iteration 800 / 1000: loss 1.587478\n",
      "iteration 900 / 1000: loss 1.576083\n",
      "For parameters:  [3.e-02 1.e+02 5.e+01]\n",
      "Validation accuracy:  0.409\n",
      "iteration 0 / 1000: loss 2.347113\n",
      "iteration 100 / 1000: loss 1.822641\n",
      "iteration 200 / 1000: loss 1.899611\n",
      "iteration 300 / 1000: loss 1.982349\n",
      "iteration 400 / 1000: loss 1.871965\n",
      "iteration 500 / 1000: loss 1.705308\n",
      "iteration 600 / 1000: loss 1.883409\n",
      "iteration 700 / 1000: loss 1.724040\n",
      "iteration 800 / 1000: loss 1.695064\n",
      "iteration 900 / 1000: loss 1.711206\n",
      "For parameters:  [3.0e-02 1.2e+02 1.0e+02]\n",
      "Validation accuracy:  0.406\n",
      "iteration 0 / 1000: loss 2.395162\n",
      "iteration 100 / 1000: loss 2.006948\n",
      "iteration 200 / 1000: loss 1.956431\n",
      "iteration 300 / 1000: loss 1.987345\n",
      "iteration 400 / 1000: loss 1.903888\n",
      "iteration 500 / 1000: loss 1.850027\n",
      "iteration 600 / 1000: loss 1.930595\n",
      "iteration 700 / 1000: loss 1.767411\n",
      "iteration 800 / 1000: loss 1.826727\n",
      "iteration 900 / 1000: loss 1.897840\n",
      "For parameters:  [3.e-02 2.e+01 2.e+02]\n",
      "Validation accuracy:  0.398\n",
      "iteration 0 / 1000: loss 2.312706\n",
      "iteration 100 / 1000: loss 1.915991\n",
      "iteration 200 / 1000: loss 1.988096\n",
      "iteration 300 / 1000: loss 1.854926\n",
      "iteration 400 / 1000: loss 1.940358\n",
      "iteration 500 / 1000: loss 1.912949\n",
      "iteration 600 / 1000: loss 1.756509\n",
      "iteration 700 / 1000: loss 1.827021\n",
      "iteration 800 / 1000: loss 1.795509\n",
      "iteration 900 / 1000: loss 1.707773\n",
      "For parameters:  [3.e-02 5.e+01 5.e+01]\n",
      "Validation accuracy:  0.407\n",
      "iteration 0 / 1000: loss 2.464060\n",
      "iteration 100 / 1000: loss 2.008343\n",
      "iteration 200 / 1000: loss 1.703615\n",
      "iteration 300 / 1000: loss 1.906412\n",
      "iteration 400 / 1000: loss 1.775457\n",
      "iteration 500 / 1000: loss 1.727726\n",
      "iteration 600 / 1000: loss 1.742025\n",
      "iteration 700 / 1000: loss 1.818305\n",
      "iteration 800 / 1000: loss 1.837705\n",
      "iteration 900 / 1000: loss 1.795398\n",
      "For parameters:  [3.e-02 8.e+01 1.e+02]\n",
      "Validation accuracy:  0.415\n",
      "iteration 0 / 1000: loss 2.453507\n",
      "iteration 100 / 1000: loss 1.888267\n",
      "iteration 200 / 1000: loss 1.846556\n",
      "iteration 300 / 1000: loss 1.869506\n",
      "iteration 400 / 1000: loss 1.778286\n",
      "iteration 500 / 1000: loss 1.812916\n",
      "iteration 600 / 1000: loss 1.840019\n",
      "iteration 700 / 1000: loss 1.590447\n",
      "iteration 800 / 1000: loss 1.676140\n",
      "iteration 900 / 1000: loss 1.559915\n",
      "For parameters:  [3.e-02 1.e+02 2.e+02]\n",
      "Validation accuracy:  0.402\n",
      "iteration 0 / 1000: loss 2.496868\n",
      "iteration 100 / 1000: loss 1.960003\n",
      "iteration 200 / 1000: loss 1.817180\n",
      "iteration 300 / 1000: loss 2.041408\n",
      "iteration 400 / 1000: loss 1.822522\n",
      "iteration 500 / 1000: loss 1.620768\n",
      "iteration 600 / 1000: loss 1.958609\n",
      "iteration 700 / 1000: loss 1.641498\n",
      "iteration 800 / 1000: loss 1.776475\n",
      "iteration 900 / 1000: loss 1.828980\n",
      "For parameters:  [3.0e-02 1.2e+02 5.0e+01]\n",
      "Validation accuracy:  0.393\n",
      "iteration 0 / 1000: loss 2.409908\n",
      "iteration 100 / 1000: loss 2.035423\n",
      "iteration 200 / 1000: loss 1.920146\n",
      "iteration 300 / 1000: loss 2.013233\n",
      "iteration 400 / 1000: loss 1.809811\n",
      "iteration 500 / 1000: loss 1.958199\n",
      "iteration 600 / 1000: loss 1.734700\n",
      "iteration 700 / 1000: loss 1.779192\n",
      "iteration 800 / 1000: loss 1.697347\n",
      "iteration 900 / 1000: loss 1.835212\n",
      "For parameters:  [3.e-02 2.e+01 1.e+02]\n",
      "Validation accuracy:  0.394\n",
      "iteration 0 / 1000: loss 2.430125\n",
      "iteration 100 / 1000: loss 1.980472\n",
      "iteration 200 / 1000: loss 1.894741\n",
      "iteration 300 / 1000: loss 1.876238\n",
      "iteration 400 / 1000: loss 1.790473\n",
      "iteration 500 / 1000: loss 1.768539\n",
      "iteration 600 / 1000: loss 1.786717\n",
      "iteration 700 / 1000: loss 1.751591\n",
      "iteration 800 / 1000: loss 1.620372\n",
      "iteration 900 / 1000: loss 1.717104\n",
      "For parameters:  [3.e-02 5.e+01 2.e+02]\n",
      "Validation accuracy:  0.408\n",
      "iteration 0 / 1000: loss 2.329865\n",
      "iteration 100 / 1000: loss 1.922829\n",
      "iteration 200 / 1000: loss 1.866684\n",
      "iteration 300 / 1000: loss 1.862252\n",
      "iteration 400 / 1000: loss 1.809355\n",
      "iteration 500 / 1000: loss 1.688395\n",
      "iteration 600 / 1000: loss 1.758873\n",
      "iteration 700 / 1000: loss 1.601702\n",
      "iteration 800 / 1000: loss 1.975365\n",
      "iteration 900 / 1000: loss 1.793599\n",
      "For parameters:  [3.e-02 8.e+01 5.e+01]\n",
      "Validation accuracy:  0.411\n",
      "iteration 0 / 1000: loss 2.532448\n",
      "iteration 100 / 1000: loss 1.974171\n",
      "iteration 200 / 1000: loss 1.714697\n",
      "iteration 300 / 1000: loss 1.844785\n",
      "iteration 400 / 1000: loss 1.886980\n",
      "iteration 500 / 1000: loss 1.673030\n",
      "iteration 600 / 1000: loss 1.871183\n",
      "iteration 700 / 1000: loss 1.729487\n",
      "iteration 800 / 1000: loss 1.915398\n",
      "iteration 900 / 1000: loss 1.631230\n",
      "For parameters:  [3.e-02 1.e+02 1.e+02]\n",
      "Validation accuracy:  0.415\n",
      "iteration 0 / 1000: loss 2.345802\n",
      "iteration 100 / 1000: loss 1.854969\n",
      "iteration 200 / 1000: loss 1.849685\n",
      "iteration 300 / 1000: loss 1.862382\n",
      "iteration 400 / 1000: loss 1.733506\n",
      "iteration 500 / 1000: loss 1.847152\n",
      "iteration 600 / 1000: loss 1.769355\n",
      "iteration 700 / 1000: loss 1.759110\n",
      "iteration 800 / 1000: loss 1.783592\n",
      "iteration 900 / 1000: loss 1.725988\n",
      "For parameters:  [3.0e-02 1.2e+02 2.0e+02]\n",
      "Validation accuracy:  0.406\n",
      "The best combination of parameters is: learning rate =  0.1 , hidden layer size =  80 , batch size =  200\n",
      "For this combination, the validation accuracy is  0.436\n"
     ]
    }
   ],
   "source": [
    "best_2layer_sigmoid = None # store the best model into this \n",
    "\n",
    "#################################################################################\n",
    "# TODO: Tune hyperparameters using the validation set. Store your best trained  #\n",
    "# model in best_2layer_sigmoid.                                                 #\n",
    "#################################################################################\n",
    "\n",
    "#Initialize \"best\" validation accuracy\n",
    "\n",
    "best_val_acc = 0\n",
    "\n",
    "#Initialize vector to store all validation accuracies\n",
    "\n",
    "val_acc_vec = np.zeros(len(parameters_matrix))\n",
    "\n",
    "#Cycle through the matrix of parameters\n",
    "\n",
    "for i in range(len(parameters_matrix)):\n",
    "\n",
    "  input_size = 32 * 32 * 3\n",
    "  num_layers = 2\n",
    "  hidden_size = int(parameters_matrix[i,1])\n",
    "  hidden_sizes = [hidden_size]*(num_layers-1)\n",
    "  num_classes = 10\n",
    "  learning_rate = parameters_matrix[i,0]\n",
    "  learning_rate_decay = 0.95\n",
    "  batch_size = int(parameters_matrix[i,2])\n",
    "  net = NeuralNetwork(input_size, hidden_sizes, num_classes, num_layers, nonlinearity='sigmoid')\n",
    "\n",
    "  # Train the network\n",
    "  stats = net.train(X_train, y_train, X_val, y_val,\n",
    "              num_iters=1000, batch_size=batch_size,\n",
    "              learning_rate=learning_rate, learning_rate_decay=learning_rate_decay,\n",
    "              reg=0.0, verbose=True)\n",
    "  \n",
    "  # Predict on the validation set\n",
    "  val_acc = (net.predict(X_val) == y_val).mean()\n",
    "\n",
    "  print('For parameters: ', parameters_matrix[i])\n",
    "  print('Validation accuracy: ', val_acc)\n",
    "\n",
    "  #Store the validaction accuracies\n",
    "\n",
    "  val_acc_vec[i] = val_acc\n",
    "\n",
    "  #Save the best model and stats\n",
    "\n",
    "  if val_acc > best_val_acc:\n",
    "\n",
    "    best_val_acc = val_acc\n",
    "    best_2layer_sigmoid = net\n",
    "    best_stats = stats\n",
    "\n",
    "#Print the best combination of trial parameters:\n",
    "\n",
    "ind = np.argmax(val_acc_vec)\n",
    "\n",
    "print('The best combination of parameters is: learning rate = ',parameters_matrix[ind,0], ', hidden layer size = ',int(parameters_matrix[ind,1]), ', batch size = ',int(parameters_matrix[ind,2]))\n",
    "print('For this combination, the validation accuracy is ', val_acc_vec[ind])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYUHCt2qVMfg"
   },
   "source": [
    "## Three-layer Relu Activation Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 462334,
     "status": "ok",
     "timestamp": 1601353169339,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "BkXQF2a9VMfh",
    "outputId": "073bcc45-e80d-4f06-ba42-7ddde7215c2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 2.488548\n",
      "iteration 100 / 1000: loss 2.149827\n",
      "iteration 200 / 1000: loss 1.861412\n",
      "iteration 300 / 1000: loss 1.573523\n",
      "iteration 400 / 1000: loss 1.660306\n",
      "iteration 500 / 1000: loss 1.895783\n",
      "iteration 600 / 1000: loss 1.484960\n",
      "iteration 700 / 1000: loss 1.507817\n",
      "iteration 800 / 1000: loss 1.812805\n",
      "iteration 900 / 1000: loss 1.652591\n",
      "For parameters:  [ 0.1 20.  50. ]\n",
      "Validation accuracy:  0.398\n",
      "iteration 0 / 1000: loss 2.506757\n",
      "iteration 100 / 1000: loss 1.790607\n",
      "iteration 200 / 1000: loss 1.535027\n",
      "iteration 300 / 1000: loss 1.468571\n",
      "iteration 400 / 1000: loss 1.512914\n",
      "iteration 500 / 1000: loss 1.590604\n",
      "iteration 600 / 1000: loss 1.384771\n",
      "iteration 700 / 1000: loss 1.651912\n",
      "iteration 800 / 1000: loss 1.385913\n",
      "iteration 900 / 1000: loss 1.587206\n",
      "For parameters:  [  0.1  50.  100. ]\n",
      "Validation accuracy:  0.459\n",
      "iteration 0 / 1000: loss 2.356532\n",
      "iteration 100 / 1000: loss 1.686930\n",
      "iteration 200 / 1000: loss 1.563930\n",
      "iteration 300 / 1000: loss 1.420197\n",
      "iteration 400 / 1000: loss 1.643392\n",
      "iteration 500 / 1000: loss 1.403180\n",
      "iteration 600 / 1000: loss 1.398474\n",
      "iteration 700 / 1000: loss 1.289954\n",
      "iteration 800 / 1000: loss 1.273592\n",
      "iteration 900 / 1000: loss 1.416335\n",
      "For parameters:  [1.e-01 8.e+01 2.e+02]\n",
      "Validation accuracy:  0.49\n",
      "iteration 0 / 1000: loss 2.427977\n",
      "iteration 100 / 1000: loss 1.795480\n",
      "iteration 200 / 1000: loss 1.683010\n",
      "iteration 300 / 1000: loss 1.726709\n",
      "iteration 400 / 1000: loss 1.568488\n",
      "iteration 500 / 1000: loss 1.400907\n",
      "iteration 600 / 1000: loss 1.580139\n",
      "iteration 700 / 1000: loss 1.256653\n",
      "iteration 800 / 1000: loss 1.615023\n",
      "iteration 900 / 1000: loss 1.370663\n",
      "For parameters:  [  0.1 100.   50. ]\n",
      "Validation accuracy:  0.447\n",
      "iteration 0 / 1000: loss 2.368715\n",
      "iteration 100 / 1000: loss 1.690701\n",
      "iteration 200 / 1000: loss 1.571183\n",
      "iteration 300 / 1000: loss 1.525240\n",
      "iteration 400 / 1000: loss 1.494508\n",
      "iteration 500 / 1000: loss 1.591508\n",
      "iteration 600 / 1000: loss 1.668100\n",
      "iteration 700 / 1000: loss 1.352411\n",
      "iteration 800 / 1000: loss 1.368516\n",
      "iteration 900 / 1000: loss 1.499359\n",
      "For parameters:  [1.0e-01 1.2e+02 1.0e+02]\n",
      "Validation accuracy:  0.503\n",
      "iteration 0 / 1000: loss 2.366797\n",
      "iteration 100 / 1000: loss 1.871385\n",
      "iteration 200 / 1000: loss 1.643839\n",
      "iteration 300 / 1000: loss 1.508153\n",
      "iteration 400 / 1000: loss 1.581833\n",
      "iteration 500 / 1000: loss 1.568563\n",
      "iteration 600 / 1000: loss 1.560668\n",
      "iteration 700 / 1000: loss 1.669056\n",
      "iteration 800 / 1000: loss 1.464973\n",
      "iteration 900 / 1000: loss 1.552902\n",
      "For parameters:  [1.e-01 2.e+01 2.e+02]\n",
      "Validation accuracy:  0.441\n",
      "iteration 0 / 1000: loss 2.410853\n",
      "iteration 100 / 1000: loss 1.771586\n",
      "iteration 200 / 1000: loss 2.127667\n",
      "iteration 300 / 1000: loss 1.591124\n",
      "iteration 400 / 1000: loss 1.640894\n",
      "iteration 500 / 1000: loss 1.707135\n",
      "iteration 600 / 1000: loss 1.651658\n",
      "iteration 700 / 1000: loss 1.807850\n",
      "iteration 800 / 1000: loss 1.491906\n",
      "iteration 900 / 1000: loss 1.324043\n",
      "For parameters:  [ 0.1 50.  50. ]\n",
      "Validation accuracy:  0.437\n",
      "iteration 0 / 1000: loss 2.396920\n",
      "iteration 100 / 1000: loss 1.857339\n",
      "iteration 200 / 1000: loss 1.536795\n",
      "iteration 300 / 1000: loss 1.698601\n",
      "iteration 400 / 1000: loss 1.466450\n",
      "iteration 500 / 1000: loss 1.408033\n",
      "iteration 600 / 1000: loss 1.547902\n",
      "iteration 700 / 1000: loss 1.691444\n",
      "iteration 800 / 1000: loss 1.418001\n",
      "iteration 900 / 1000: loss 1.277043\n",
      "For parameters:  [  0.1  80.  100. ]\n",
      "Validation accuracy:  0.465\n",
      "iteration 0 / 1000: loss 2.403956\n",
      "iteration 100 / 1000: loss 1.732292\n",
      "iteration 200 / 1000: loss 1.667944\n",
      "iteration 300 / 1000: loss 1.454914\n",
      "iteration 400 / 1000: loss 1.429880\n",
      "iteration 500 / 1000: loss 1.418885\n",
      "iteration 600 / 1000: loss 1.542091\n",
      "iteration 700 / 1000: loss 1.366346\n",
      "iteration 800 / 1000: loss 1.290044\n",
      "iteration 900 / 1000: loss 1.229839\n",
      "For parameters:  [1.e-01 1.e+02 2.e+02]\n",
      "Validation accuracy:  0.507\n",
      "iteration 0 / 1000: loss 2.387579\n",
      "iteration 100 / 1000: loss 1.828161\n",
      "iteration 200 / 1000: loss 1.648334\n",
      "iteration 300 / 1000: loss 1.788637\n",
      "iteration 400 / 1000: loss 1.592732\n",
      "iteration 500 / 1000: loss 1.225307\n",
      "iteration 600 / 1000: loss 1.482539\n",
      "iteration 700 / 1000: loss 1.486220\n",
      "iteration 800 / 1000: loss 1.460764\n",
      "iteration 900 / 1000: loss 1.558186\n",
      "For parameters:  [1.0e-01 1.2e+02 5.0e+01]\n",
      "Validation accuracy:  0.45\n",
      "iteration 0 / 1000: loss 2.341265\n",
      "iteration 100 / 1000: loss 1.834678\n",
      "iteration 200 / 1000: loss 1.897337\n",
      "iteration 300 / 1000: loss 1.619981\n",
      "iteration 400 / 1000: loss 1.547761\n",
      "iteration 500 / 1000: loss 1.729191\n",
      "iteration 600 / 1000: loss 1.734293\n",
      "iteration 700 / 1000: loss 1.533691\n",
      "iteration 800 / 1000: loss 1.534677\n",
      "iteration 900 / 1000: loss 1.280966\n",
      "For parameters:  [  0.1  20.  100. ]\n",
      "Validation accuracy:  0.441\n",
      "iteration 0 / 1000: loss 2.379973\n",
      "iteration 100 / 1000: loss 1.711375\n",
      "iteration 200 / 1000: loss 1.580873\n",
      "iteration 300 / 1000: loss 1.512723\n",
      "iteration 400 / 1000: loss 1.486065\n",
      "iteration 500 / 1000: loss 1.475074\n",
      "iteration 600 / 1000: loss 1.567917\n",
      "iteration 700 / 1000: loss 1.465371\n",
      "iteration 800 / 1000: loss 1.338320\n",
      "iteration 900 / 1000: loss 1.419387\n",
      "For parameters:  [1.e-01 5.e+01 2.e+02]\n",
      "Validation accuracy:  0.492\n",
      "iteration 0 / 1000: loss 2.474014\n",
      "iteration 100 / 1000: loss 1.787571\n",
      "iteration 200 / 1000: loss 1.740740\n",
      "iteration 300 / 1000: loss 1.780470\n",
      "iteration 400 / 1000: loss 1.746044\n",
      "iteration 500 / 1000: loss 1.864854\n",
      "iteration 600 / 1000: loss 1.773115\n",
      "iteration 700 / 1000: loss 1.789275\n",
      "iteration 800 / 1000: loss 1.453697\n",
      "iteration 900 / 1000: loss 1.689362\n",
      "For parameters:  [ 0.1 80.  50. ]\n",
      "Validation accuracy:  0.44\n",
      "iteration 0 / 1000: loss 2.496058\n",
      "iteration 100 / 1000: loss 1.863945\n",
      "iteration 200 / 1000: loss 1.682928\n",
      "iteration 300 / 1000: loss 1.536935\n",
      "iteration 400 / 1000: loss 1.511178\n",
      "iteration 500 / 1000: loss 1.430720\n",
      "iteration 600 / 1000: loss 1.507224\n",
      "iteration 700 / 1000: loss 1.529159\n",
      "iteration 800 / 1000: loss 1.497969\n",
      "iteration 900 / 1000: loss 1.380143\n",
      "For parameters:  [  0.1 100.  100. ]\n",
      "Validation accuracy:  0.475\n",
      "iteration 0 / 1000: loss 2.435805\n",
      "iteration 100 / 1000: loss 1.635913\n",
      "iteration 200 / 1000: loss 1.724216\n",
      "iteration 300 / 1000: loss 1.564232\n",
      "iteration 400 / 1000: loss 1.522211\n",
      "iteration 500 / 1000: loss 1.524997\n",
      "iteration 600 / 1000: loss 1.324900\n",
      "iteration 700 / 1000: loss 1.413647\n",
      "iteration 800 / 1000: loss 1.249312\n",
      "iteration 900 / 1000: loss 1.269321\n",
      "For parameters:  [1.0e-01 1.2e+02 2.0e+02]\n",
      "Validation accuracy:  0.477\n",
      "iteration 0 / 1000: loss 2.483989\n",
      "iteration 100 / 1000: loss 1.850517\n",
      "iteration 200 / 1000: loss 1.908296\n",
      "iteration 300 / 1000: loss 1.808306\n",
      "iteration 400 / 1000: loss 1.622488\n",
      "iteration 500 / 1000: loss 1.725570\n",
      "iteration 600 / 1000: loss 1.549689\n",
      "iteration 700 / 1000: loss 1.715273\n",
      "iteration 800 / 1000: loss 1.739351\n",
      "iteration 900 / 1000: loss 1.736076\n",
      "For parameters:  [ 0.05 20.   50.  ]\n",
      "Validation accuracy:  0.422\n",
      "iteration 0 / 1000: loss 2.478430\n",
      "iteration 100 / 1000: loss 1.807153\n",
      "iteration 200 / 1000: loss 1.873974\n",
      "iteration 300 / 1000: loss 1.626945\n",
      "iteration 400 / 1000: loss 1.464768\n",
      "iteration 500 / 1000: loss 1.438573\n",
      "iteration 600 / 1000: loss 1.444805\n",
      "iteration 700 / 1000: loss 1.695742\n",
      "iteration 800 / 1000: loss 1.380290\n",
      "iteration 900 / 1000: loss 1.434087\n",
      "For parameters:  [5.e-02 5.e+01 1.e+02]\n",
      "Validation accuracy:  0.466\n",
      "iteration 0 / 1000: loss 2.467992\n",
      "iteration 100 / 1000: loss 1.655150\n",
      "iteration 200 / 1000: loss 1.604073\n",
      "iteration 300 / 1000: loss 1.459405\n",
      "iteration 400 / 1000: loss 1.435752\n",
      "iteration 500 / 1000: loss 1.382560\n",
      "iteration 600 / 1000: loss 1.352078\n",
      "iteration 700 / 1000: loss 1.266220\n",
      "iteration 800 / 1000: loss 1.358554\n",
      "iteration 900 / 1000: loss 1.434934\n",
      "For parameters:  [5.e-02 8.e+01 2.e+02]\n",
      "Validation accuracy:  0.481\n",
      "iteration 0 / 1000: loss 2.389696\n",
      "iteration 100 / 1000: loss 2.028125\n",
      "iteration 200 / 1000: loss 1.573349\n",
      "iteration 300 / 1000: loss 1.424485\n",
      "iteration 400 / 1000: loss 1.756657\n",
      "iteration 500 / 1000: loss 1.604768\n",
      "iteration 600 / 1000: loss 1.816790\n",
      "iteration 700 / 1000: loss 1.598568\n",
      "iteration 800 / 1000: loss 1.417245\n",
      "iteration 900 / 1000: loss 1.768143\n",
      "For parameters:  [5.e-02 1.e+02 5.e+01]\n",
      "Validation accuracy:  0.458\n",
      "iteration 0 / 1000: loss 2.477916\n",
      "iteration 100 / 1000: loss 1.628520\n",
      "iteration 200 / 1000: loss 1.738098\n",
      "iteration 300 / 1000: loss 1.715109\n",
      "iteration 400 / 1000: loss 1.606572\n",
      "iteration 500 / 1000: loss 1.390299\n",
      "iteration 600 / 1000: loss 1.504065\n",
      "iteration 700 / 1000: loss 1.485633\n",
      "iteration 800 / 1000: loss 1.484221\n",
      "iteration 900 / 1000: loss 1.408944\n",
      "For parameters:  [5.0e-02 1.2e+02 1.0e+02]\n",
      "Validation accuracy:  0.46\n",
      "iteration 0 / 1000: loss 2.378349\n",
      "iteration 100 / 1000: loss 1.798541\n",
      "iteration 200 / 1000: loss 1.741227\n",
      "iteration 300 / 1000: loss 1.744580\n",
      "iteration 400 / 1000: loss 1.682288\n",
      "iteration 500 / 1000: loss 1.612828\n",
      "iteration 600 / 1000: loss 1.647541\n",
      "iteration 700 / 1000: loss 1.564386\n",
      "iteration 800 / 1000: loss 1.521308\n",
      "iteration 900 / 1000: loss 1.496103\n",
      "For parameters:  [5.e-02 2.e+01 2.e+02]\n",
      "Validation accuracy:  0.435\n",
      "iteration 0 / 1000: loss 2.403335\n",
      "iteration 100 / 1000: loss 1.760945\n",
      "iteration 200 / 1000: loss 1.788317\n",
      "iteration 300 / 1000: loss 1.673200\n",
      "iteration 400 / 1000: loss 1.759980\n",
      "iteration 500 / 1000: loss 1.639813\n",
      "iteration 600 / 1000: loss 1.630916\n",
      "iteration 700 / 1000: loss 1.753673\n",
      "iteration 800 / 1000: loss 1.867768\n",
      "iteration 900 / 1000: loss 1.692217\n",
      "For parameters:  [ 0.05 50.   50.  ]\n",
      "Validation accuracy:  0.447\n",
      "iteration 0 / 1000: loss 2.461821\n",
      "iteration 100 / 1000: loss 1.652415\n",
      "iteration 200 / 1000: loss 1.618583\n",
      "iteration 300 / 1000: loss 1.562309\n",
      "iteration 400 / 1000: loss 1.530729\n",
      "iteration 500 / 1000: loss 1.476320\n",
      "iteration 600 / 1000: loss 1.678424\n",
      "iteration 700 / 1000: loss 1.483633\n",
      "iteration 800 / 1000: loss 1.443606\n",
      "iteration 900 / 1000: loss 1.396224\n",
      "For parameters:  [5.e-02 8.e+01 1.e+02]\n",
      "Validation accuracy:  0.477\n",
      "iteration 0 / 1000: loss 2.352702\n",
      "iteration 100 / 1000: loss 1.752401\n",
      "iteration 200 / 1000: loss 1.647341\n",
      "iteration 300 / 1000: loss 1.613936\n",
      "iteration 400 / 1000: loss 1.540483\n",
      "iteration 500 / 1000: loss 1.379023\n",
      "iteration 600 / 1000: loss 1.421652\n",
      "iteration 700 / 1000: loss 1.408681\n",
      "iteration 800 / 1000: loss 1.390706\n",
      "iteration 900 / 1000: loss 1.262250\n",
      "For parameters:  [5.e-02 1.e+02 2.e+02]\n",
      "Validation accuracy:  0.492\n",
      "iteration 0 / 1000: loss 2.531880\n",
      "iteration 100 / 1000: loss 1.759208\n",
      "iteration 200 / 1000: loss 1.777803\n",
      "iteration 300 / 1000: loss 1.403432\n",
      "iteration 400 / 1000: loss 1.411589\n",
      "iteration 500 / 1000: loss 1.783714\n",
      "iteration 600 / 1000: loss 1.325228\n",
      "iteration 700 / 1000: loss 1.551734\n",
      "iteration 800 / 1000: loss 1.339371\n",
      "iteration 900 / 1000: loss 1.531675\n",
      "For parameters:  [5.0e-02 1.2e+02 5.0e+01]\n",
      "Validation accuracy:  0.461\n",
      "iteration 0 / 1000: loss 2.417943\n",
      "iteration 100 / 1000: loss 1.767447\n",
      "iteration 200 / 1000: loss 1.631187\n",
      "iteration 300 / 1000: loss 1.584551\n",
      "iteration 400 / 1000: loss 1.649359\n",
      "iteration 500 / 1000: loss 1.602876\n",
      "iteration 600 / 1000: loss 1.657365\n",
      "iteration 700 / 1000: loss 1.596195\n",
      "iteration 800 / 1000: loss 1.555987\n",
      "iteration 900 / 1000: loss 1.501673\n",
      "For parameters:  [5.e-02 2.e+01 1.e+02]\n",
      "Validation accuracy:  0.411\n",
      "iteration 0 / 1000: loss 2.462928\n",
      "iteration 100 / 1000: loss 1.800741\n",
      "iteration 200 / 1000: loss 1.657112\n",
      "iteration 300 / 1000: loss 1.605263\n",
      "iteration 400 / 1000: loss 1.610468\n",
      "iteration 500 / 1000: loss 1.592227\n",
      "iteration 600 / 1000: loss 1.465059\n",
      "iteration 700 / 1000: loss 1.428843\n",
      "iteration 800 / 1000: loss 1.554932\n",
      "iteration 900 / 1000: loss 1.430861\n",
      "For parameters:  [5.e-02 5.e+01 2.e+02]\n",
      "Validation accuracy:  0.463\n",
      "iteration 0 / 1000: loss 2.319795\n",
      "iteration 100 / 1000: loss 1.750372\n",
      "iteration 200 / 1000: loss 1.892024\n",
      "iteration 300 / 1000: loss 1.643956\n",
      "iteration 400 / 1000: loss 1.566700\n",
      "iteration 500 / 1000: loss 1.775872\n",
      "iteration 600 / 1000: loss 1.392110\n",
      "iteration 700 / 1000: loss 1.513182\n",
      "iteration 800 / 1000: loss 1.620749\n",
      "iteration 900 / 1000: loss 1.560164\n",
      "For parameters:  [5.e-02 8.e+01 5.e+01]\n",
      "Validation accuracy:  0.451\n",
      "iteration 0 / 1000: loss 2.454141\n",
      "iteration 100 / 1000: loss 1.579955\n",
      "iteration 200 / 1000: loss 1.642354\n",
      "iteration 300 / 1000: loss 1.650825\n",
      "iteration 400 / 1000: loss 1.715080\n",
      "iteration 500 / 1000: loss 1.457042\n",
      "iteration 600 / 1000: loss 1.522399\n",
      "iteration 700 / 1000: loss 1.478631\n",
      "iteration 800 / 1000: loss 1.516866\n",
      "iteration 900 / 1000: loss 1.260067\n",
      "For parameters:  [5.e-02 1.e+02 1.e+02]\n",
      "Validation accuracy:  0.495\n",
      "iteration 0 / 1000: loss 2.386968\n",
      "iteration 100 / 1000: loss 1.596986\n",
      "iteration 200 / 1000: loss 1.554853\n",
      "iteration 300 / 1000: loss 1.583574\n",
      "iteration 400 / 1000: loss 1.402773\n",
      "iteration 500 / 1000: loss 1.378430\n",
      "iteration 600 / 1000: loss 1.375337\n",
      "iteration 700 / 1000: loss 1.315856\n",
      "iteration 800 / 1000: loss 1.275970\n",
      "iteration 900 / 1000: loss 1.365006\n",
      "For parameters:  [5.0e-02 1.2e+02 2.0e+02]\n",
      "Validation accuracy:  0.501\n",
      "iteration 0 / 1000: loss 2.448250\n",
      "iteration 100 / 1000: loss 2.110432\n",
      "iteration 200 / 1000: loss 1.665533\n",
      "iteration 300 / 1000: loss 1.843127\n",
      "iteration 400 / 1000: loss 1.807560\n",
      "iteration 500 / 1000: loss 1.773654\n",
      "iteration 600 / 1000: loss 1.761658\n",
      "iteration 700 / 1000: loss 1.285798\n",
      "iteration 800 / 1000: loss 1.898557\n",
      "iteration 900 / 1000: loss 1.426165\n",
      "For parameters:  [3.e-02 2.e+01 5.e+01]\n",
      "Validation accuracy:  0.408\n",
      "iteration 0 / 1000: loss 2.422844\n",
      "iteration 100 / 1000: loss 1.886660\n",
      "iteration 200 / 1000: loss 1.700300\n",
      "iteration 300 / 1000: loss 1.896407\n",
      "iteration 400 / 1000: loss 1.371108\n",
      "iteration 500 / 1000: loss 1.518603\n",
      "iteration 600 / 1000: loss 1.491910\n",
      "iteration 700 / 1000: loss 1.587119\n",
      "iteration 800 / 1000: loss 1.451214\n",
      "iteration 900 / 1000: loss 1.448096\n",
      "For parameters:  [3.e-02 5.e+01 1.e+02]\n",
      "Validation accuracy:  0.461\n",
      "iteration 0 / 1000: loss 2.432160\n",
      "iteration 100 / 1000: loss 1.759717\n",
      "iteration 200 / 1000: loss 1.637465\n",
      "iteration 300 / 1000: loss 1.685979\n",
      "iteration 400 / 1000: loss 1.483603\n",
      "iteration 500 / 1000: loss 1.545669\n",
      "iteration 600 / 1000: loss 1.441953\n",
      "iteration 700 / 1000: loss 1.512083\n",
      "iteration 800 / 1000: loss 1.470915\n",
      "iteration 900 / 1000: loss 1.430387\n",
      "For parameters:  [3.e-02 8.e+01 2.e+02]\n",
      "Validation accuracy:  0.488\n",
      "iteration 0 / 1000: loss 2.475252\n",
      "iteration 100 / 1000: loss 1.811449\n",
      "iteration 200 / 1000: loss 1.636489\n",
      "iteration 300 / 1000: loss 1.640049\n",
      "iteration 400 / 1000: loss 1.721024\n",
      "iteration 500 / 1000: loss 1.778773\n",
      "iteration 600 / 1000: loss 1.373376\n",
      "iteration 700 / 1000: loss 1.557382\n",
      "iteration 800 / 1000: loss 1.594996\n",
      "iteration 900 / 1000: loss 1.253603\n",
      "For parameters:  [3.e-02 1.e+02 5.e+01]\n",
      "Validation accuracy:  0.452\n",
      "iteration 0 / 1000: loss 2.435103\n",
      "iteration 100 / 1000: loss 1.801877\n",
      "iteration 200 / 1000: loss 1.673828\n",
      "iteration 300 / 1000: loss 1.692316\n",
      "iteration 400 / 1000: loss 1.661874\n",
      "iteration 500 / 1000: loss 1.472946\n",
      "iteration 600 / 1000: loss 1.600589\n",
      "iteration 700 / 1000: loss 1.433157\n",
      "iteration 800 / 1000: loss 1.437748\n",
      "iteration 900 / 1000: loss 1.228969\n",
      "For parameters:  [3.0e-02 1.2e+02 1.0e+02]\n",
      "Validation accuracy:  0.479\n",
      "iteration 0 / 1000: loss 2.347293\n",
      "iteration 100 / 1000: loss 1.899067\n",
      "iteration 200 / 1000: loss 1.753977\n",
      "iteration 300 / 1000: loss 1.684895\n",
      "iteration 400 / 1000: loss 1.737257\n",
      "iteration 500 / 1000: loss 1.698016\n",
      "iteration 600 / 1000: loss 1.694025\n",
      "iteration 700 / 1000: loss 1.521366\n",
      "iteration 800 / 1000: loss 1.605131\n",
      "iteration 900 / 1000: loss 1.594005\n",
      "For parameters:  [3.e-02 2.e+01 2.e+02]\n",
      "Validation accuracy:  0.435\n",
      "iteration 0 / 1000: loss 2.389172\n",
      "iteration 100 / 1000: loss 1.943899\n",
      "iteration 200 / 1000: loss 1.964574\n",
      "iteration 300 / 1000: loss 1.655812\n",
      "iteration 400 / 1000: loss 1.687209\n",
      "iteration 500 / 1000: loss 1.422541\n",
      "iteration 600 / 1000: loss 1.659894\n",
      "iteration 700 / 1000: loss 1.809398\n",
      "iteration 800 / 1000: loss 1.628091\n",
      "iteration 900 / 1000: loss 1.548941\n",
      "For parameters:  [3.e-02 5.e+01 5.e+01]\n",
      "Validation accuracy:  0.447\n",
      "iteration 0 / 1000: loss 2.413365\n",
      "iteration 100 / 1000: loss 1.866576\n",
      "iteration 200 / 1000: loss 1.730257\n",
      "iteration 300 / 1000: loss 1.662296\n",
      "iteration 400 / 1000: loss 1.563400\n",
      "iteration 500 / 1000: loss 1.623761\n",
      "iteration 600 / 1000: loss 1.692550\n",
      "iteration 700 / 1000: loss 1.285304\n",
      "iteration 800 / 1000: loss 1.429589\n",
      "iteration 900 / 1000: loss 1.559710\n",
      "For parameters:  [3.e-02 8.e+01 1.e+02]\n",
      "Validation accuracy:  0.481\n",
      "iteration 0 / 1000: loss 2.369549\n",
      "iteration 100 / 1000: loss 1.744381\n",
      "iteration 200 / 1000: loss 1.712059\n",
      "iteration 300 / 1000: loss 1.589181\n",
      "iteration 400 / 1000: loss 1.518529\n",
      "iteration 500 / 1000: loss 1.573149\n",
      "iteration 600 / 1000: loss 1.398003\n",
      "iteration 700 / 1000: loss 1.294511\n",
      "iteration 800 / 1000: loss 1.483313\n",
      "iteration 900 / 1000: loss 1.467256\n",
      "For parameters:  [3.e-02 1.e+02 2.e+02]\n",
      "Validation accuracy:  0.48\n",
      "iteration 0 / 1000: loss 2.433897\n",
      "iteration 100 / 1000: loss 1.942873\n",
      "iteration 200 / 1000: loss 1.849292\n",
      "iteration 300 / 1000: loss 1.610350\n",
      "iteration 400 / 1000: loss 1.602557\n",
      "iteration 500 / 1000: loss 1.213208\n",
      "iteration 600 / 1000: loss 1.624326\n",
      "iteration 700 / 1000: loss 1.665147\n",
      "iteration 800 / 1000: loss 1.373463\n",
      "iteration 900 / 1000: loss 1.390888\n",
      "For parameters:  [3.0e-02 1.2e+02 5.0e+01]\n",
      "Validation accuracy:  0.459\n",
      "iteration 0 / 1000: loss 2.549713\n",
      "iteration 100 / 1000: loss 1.921282\n",
      "iteration 200 / 1000: loss 1.726941\n",
      "iteration 300 / 1000: loss 1.819238\n",
      "iteration 400 / 1000: loss 1.521763\n",
      "iteration 500 / 1000: loss 1.688051\n",
      "iteration 600 / 1000: loss 1.847164\n",
      "iteration 700 / 1000: loss 1.636834\n",
      "iteration 800 / 1000: loss 1.468065\n",
      "iteration 900 / 1000: loss 1.574748\n",
      "For parameters:  [3.e-02 2.e+01 1.e+02]\n",
      "Validation accuracy:  0.423\n",
      "iteration 0 / 1000: loss 2.419260\n",
      "iteration 100 / 1000: loss 1.816838\n",
      "iteration 200 / 1000: loss 1.684026\n",
      "iteration 300 / 1000: loss 1.623757\n",
      "iteration 400 / 1000: loss 1.623373\n",
      "iteration 500 / 1000: loss 1.496840\n",
      "iteration 600 / 1000: loss 1.574236\n",
      "iteration 700 / 1000: loss 1.358700\n",
      "iteration 800 / 1000: loss 1.518648\n",
      "iteration 900 / 1000: loss 1.544579\n",
      "For parameters:  [3.e-02 5.e+01 2.e+02]\n",
      "Validation accuracy:  0.485\n",
      "iteration 0 / 1000: loss 2.389910\n",
      "iteration 100 / 1000: loss 1.677383\n",
      "iteration 200 / 1000: loss 1.815267\n",
      "iteration 300 / 1000: loss 1.723192\n",
      "iteration 400 / 1000: loss 1.675641\n",
      "iteration 500 / 1000: loss 1.700449\n",
      "iteration 600 / 1000: loss 1.626160\n",
      "iteration 700 / 1000: loss 1.608528\n",
      "iteration 800 / 1000: loss 1.574901\n",
      "iteration 900 / 1000: loss 1.697080\n",
      "For parameters:  [3.e-02 8.e+01 5.e+01]\n",
      "Validation accuracy:  0.441\n",
      "iteration 0 / 1000: loss 2.412420\n",
      "iteration 100 / 1000: loss 1.898045\n",
      "iteration 200 / 1000: loss 1.734733\n",
      "iteration 300 / 1000: loss 1.649473\n",
      "iteration 400 / 1000: loss 1.453475\n",
      "iteration 500 / 1000: loss 1.713627\n",
      "iteration 600 / 1000: loss 1.467008\n",
      "iteration 700 / 1000: loss 1.527426\n",
      "iteration 800 / 1000: loss 1.672172\n",
      "iteration 900 / 1000: loss 1.361769\n",
      "For parameters:  [3.e-02 1.e+02 1.e+02]\n",
      "Validation accuracy:  0.475\n",
      "iteration 0 / 1000: loss 2.315362\n",
      "iteration 100 / 1000: loss 1.653302\n",
      "iteration 200 / 1000: loss 1.606265\n",
      "iteration 300 / 1000: loss 1.582040\n",
      "iteration 400 / 1000: loss 1.599255\n",
      "iteration 500 / 1000: loss 1.589134\n",
      "iteration 600 / 1000: loss 1.472316\n",
      "iteration 700 / 1000: loss 1.517007\n",
      "iteration 800 / 1000: loss 1.286834\n",
      "iteration 900 / 1000: loss 1.183258\n",
      "For parameters:  [3.0e-02 1.2e+02 2.0e+02]\n",
      "Validation accuracy:  0.478\n",
      "The best combination of parameters is: learning rate =  0.1 , hidden layer size =  100 , batch size =  200\n",
      "For this combination, the validation accuracy is  0.507\n"
     ]
    }
   ],
   "source": [
    "best_3layer_relu = None # store the best model into this \n",
    "\n",
    "#################################################################################\n",
    "# TODO: Tune hyperparameters using the validation set. Store your best trained  #\n",
    "# model in best_3layer_relu.                                                    #\n",
    "#################################################################################\n",
    "\n",
    "#Initialize \"best\" validation accuracy\n",
    "\n",
    "best_val_acc = 0\n",
    "\n",
    "#Initialize vector to store all validation accuracies\n",
    "\n",
    "val_acc_vec = np.zeros(len(parameters_matrix))\n",
    "\n",
    "#Cycle through the matrix of parameters\n",
    "\n",
    "for i in range(len(parameters_matrix)):\n",
    "\n",
    "  input_size = 32 * 32 * 3\n",
    "  num_layers = 3\n",
    "  hidden_size = int(parameters_matrix[i,1])\n",
    "  hidden_sizes = [hidden_size]*(num_layers-1)\n",
    "  num_classes = 10\n",
    "  learning_rate = parameters_matrix[i,0]\n",
    "  learning_rate_decay = 0.95\n",
    "  batch_size = int(parameters_matrix[i,2])\n",
    "  net = NeuralNetwork(input_size, hidden_sizes, num_classes, num_layers, nonlinearity='relu')\n",
    "\n",
    "  # Train the network\n",
    "  stats = net.train(X_train, y_train, X_val, y_val,\n",
    "              num_iters=1000, batch_size=batch_size,\n",
    "              learning_rate=learning_rate, learning_rate_decay=learning_rate_decay,\n",
    "              reg=0.0, verbose=True)\n",
    "  \n",
    "  # Predict on the validation set\n",
    "  val_acc = (net.predict(X_val) == y_val).mean()\n",
    "\n",
    "  print('For parameters: ', parameters_matrix[i])\n",
    "  print('Validation accuracy: ', val_acc)\n",
    "\n",
    "  #Store the validaction accuracies\n",
    "\n",
    "  val_acc_vec[i] = val_acc\n",
    "\n",
    "  #Save the best model and stats\n",
    "\n",
    "  if val_acc > best_val_acc:\n",
    "\n",
    "    best_val_acc = val_acc\n",
    "    best_3layer_relu = net\n",
    "    best_stats = stats\n",
    "\n",
    "#Print the best combination of trial parameters:\n",
    "\n",
    "ind = np.argmax(val_acc_vec)\n",
    "\n",
    "print('The best combination of parameters is: learning rate = ',parameters_matrix[ind,0], ', hidden layer size = ',int(parameters_matrix[ind,1]), ', batch size = ',int(parameters_matrix[ind,2]))\n",
    "print('For this combination, the validation accuracy is ', val_acc_vec[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8k-vnx8JVMfj"
   },
   "source": [
    "## Three-layer Sigmoid Activation Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 534430,
     "status": "ok",
     "timestamp": 1601353775561,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "9oYlypdJVMfj",
    "outputId": "001d3161-7596-42b5-9a41-9533e19f40ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 2.341498\n",
      "iteration 100 / 1000: loss 2.207852\n",
      "iteration 200 / 1000: loss 2.139997\n",
      "iteration 300 / 1000: loss 2.051488\n",
      "iteration 400 / 1000: loss 2.049709\n",
      "iteration 500 / 1000: loss 2.104036\n",
      "iteration 600 / 1000: loss 2.050512\n",
      "iteration 700 / 1000: loss 1.851410\n",
      "iteration 800 / 1000: loss 1.940754\n",
      "iteration 900 / 1000: loss 2.018969\n",
      "For parameters:  [ 0.1 20.  50. ]\n",
      "Validation accuracy:  0.357\n",
      "iteration 0 / 1000: loss 2.448612\n",
      "iteration 100 / 1000: loss 2.077613\n",
      "iteration 200 / 1000: loss 2.039883\n",
      "iteration 300 / 1000: loss 2.033507\n",
      "iteration 400 / 1000: loss 1.895880\n",
      "iteration 500 / 1000: loss 1.932328\n",
      "iteration 600 / 1000: loss 1.925898\n",
      "iteration 700 / 1000: loss 1.838447\n",
      "iteration 800 / 1000: loss 1.856493\n",
      "iteration 900 / 1000: loss 1.591386\n",
      "For parameters:  [  0.1  50.  100. ]\n",
      "Validation accuracy:  0.386\n",
      "iteration 0 / 1000: loss 2.364265\n",
      "iteration 100 / 1000: loss 2.081808\n",
      "iteration 200 / 1000: loss 2.016848\n",
      "iteration 300 / 1000: loss 1.931935\n",
      "iteration 400 / 1000: loss 1.859091\n",
      "iteration 500 / 1000: loss 1.844422\n",
      "iteration 600 / 1000: loss 1.795662\n",
      "iteration 700 / 1000: loss 1.825233\n",
      "iteration 800 / 1000: loss 1.681379\n",
      "iteration 900 / 1000: loss 1.788236\n",
      "For parameters:  [1.e-01 8.e+01 2.e+02]\n",
      "Validation accuracy:  0.414\n",
      "iteration 0 / 1000: loss 2.413968\n",
      "iteration 100 / 1000: loss 2.127262\n",
      "iteration 200 / 1000: loss 2.028107\n",
      "iteration 300 / 1000: loss 1.918618\n",
      "iteration 400 / 1000: loss 1.834029\n",
      "iteration 500 / 1000: loss 1.918868\n",
      "iteration 600 / 1000: loss 1.710111\n",
      "iteration 700 / 1000: loss 1.979391\n",
      "iteration 800 / 1000: loss 1.606796\n",
      "iteration 900 / 1000: loss 1.870395\n",
      "For parameters:  [  0.1 100.   50. ]\n",
      "Validation accuracy:  0.402\n",
      "iteration 0 / 1000: loss 2.389077\n",
      "iteration 100 / 1000: loss 2.102345\n",
      "iteration 200 / 1000: loss 1.956554\n",
      "iteration 300 / 1000: loss 1.985965\n",
      "iteration 400 / 1000: loss 1.777410\n",
      "iteration 500 / 1000: loss 1.974525\n",
      "iteration 600 / 1000: loss 1.737353\n",
      "iteration 700 / 1000: loss 1.668831\n",
      "iteration 800 / 1000: loss 1.772797\n",
      "iteration 900 / 1000: loss 1.746701\n",
      "For parameters:  [1.0e-01 1.2e+02 1.0e+02]\n",
      "Validation accuracy:  0.384\n",
      "iteration 0 / 1000: loss 2.340397\n",
      "iteration 100 / 1000: loss 2.210570\n",
      "iteration 200 / 1000: loss 2.149262\n",
      "iteration 300 / 1000: loss 2.050630\n",
      "iteration 400 / 1000: loss 2.020849\n",
      "iteration 500 / 1000: loss 1.944951\n",
      "iteration 600 / 1000: loss 1.877223\n",
      "iteration 700 / 1000: loss 1.891415\n",
      "iteration 800 / 1000: loss 1.842731\n",
      "iteration 900 / 1000: loss 1.780380\n",
      "For parameters:  [1.e-01 2.e+01 2.e+02]\n",
      "Validation accuracy:  0.364\n",
      "iteration 0 / 1000: loss 2.370122\n",
      "iteration 100 / 1000: loss 2.148976\n",
      "iteration 200 / 1000: loss 2.010571\n",
      "iteration 300 / 1000: loss 2.057767\n",
      "iteration 400 / 1000: loss 1.919012\n",
      "iteration 500 / 1000: loss 1.860668\n",
      "iteration 600 / 1000: loss 1.922120\n",
      "iteration 700 / 1000: loss 1.898974\n",
      "iteration 800 / 1000: loss 1.886093\n",
      "iteration 900 / 1000: loss 1.821627\n",
      "For parameters:  [ 0.1 50.  50. ]\n",
      "Validation accuracy:  0.374\n",
      "iteration 0 / 1000: loss 2.479070\n",
      "iteration 100 / 1000: loss 2.042379\n",
      "iteration 200 / 1000: loss 2.066532\n",
      "iteration 300 / 1000: loss 1.969494\n",
      "iteration 400 / 1000: loss 1.981710\n",
      "iteration 500 / 1000: loss 1.830800\n",
      "iteration 600 / 1000: loss 1.884657\n",
      "iteration 700 / 1000: loss 1.804319\n",
      "iteration 800 / 1000: loss 1.784642\n",
      "iteration 900 / 1000: loss 1.819560\n",
      "For parameters:  [  0.1  80.  100. ]\n",
      "Validation accuracy:  0.379\n",
      "iteration 0 / 1000: loss 2.458014\n",
      "iteration 100 / 1000: loss 2.117269\n",
      "iteration 200 / 1000: loss 2.002774\n",
      "iteration 300 / 1000: loss 1.930603\n",
      "iteration 400 / 1000: loss 1.880399\n",
      "iteration 500 / 1000: loss 1.828451\n",
      "iteration 600 / 1000: loss 1.828094\n",
      "iteration 700 / 1000: loss 1.844087\n",
      "iteration 800 / 1000: loss 1.813307\n",
      "iteration 900 / 1000: loss 1.770259\n",
      "For parameters:  [1.e-01 1.e+02 2.e+02]\n",
      "Validation accuracy:  0.371\n",
      "iteration 0 / 1000: loss 2.481327\n",
      "iteration 100 / 1000: loss 2.065398\n",
      "iteration 200 / 1000: loss 1.981070\n",
      "iteration 300 / 1000: loss 1.781368\n",
      "iteration 400 / 1000: loss 1.890408\n",
      "iteration 500 / 1000: loss 1.905495\n",
      "iteration 600 / 1000: loss 1.839487\n",
      "iteration 700 / 1000: loss 1.854714\n",
      "iteration 800 / 1000: loss 1.843192\n",
      "iteration 900 / 1000: loss 1.760606\n",
      "For parameters:  [1.0e-01 1.2e+02 5.0e+01]\n",
      "Validation accuracy:  0.406\n",
      "iteration 0 / 1000: loss 2.421596\n",
      "iteration 100 / 1000: loss 2.220491\n",
      "iteration 200 / 1000: loss 2.189993\n",
      "iteration 300 / 1000: loss 2.026762\n",
      "iteration 400 / 1000: loss 2.038135\n",
      "iteration 500 / 1000: loss 2.111229\n",
      "iteration 600 / 1000: loss 1.932581\n",
      "iteration 700 / 1000: loss 1.892606\n",
      "iteration 800 / 1000: loss 1.821492\n",
      "iteration 900 / 1000: loss 1.926973\n",
      "For parameters:  [  0.1  20.  100. ]\n",
      "Validation accuracy:  0.355\n",
      "iteration 0 / 1000: loss 2.349100\n",
      "iteration 100 / 1000: loss 2.099988\n",
      "iteration 200 / 1000: loss 2.061398\n",
      "iteration 300 / 1000: loss 1.875662\n",
      "iteration 400 / 1000: loss 1.934079\n",
      "iteration 500 / 1000: loss 1.871254\n",
      "iteration 600 / 1000: loss 1.806076\n",
      "iteration 700 / 1000: loss 1.941981\n",
      "iteration 800 / 1000: loss 1.917081\n",
      "iteration 900 / 1000: loss 1.861327\n",
      "For parameters:  [1.e-01 5.e+01 2.e+02]\n",
      "Validation accuracy:  0.382\n",
      "iteration 0 / 1000: loss 2.370517\n",
      "iteration 100 / 1000: loss 2.115029\n",
      "iteration 200 / 1000: loss 2.169580\n",
      "iteration 300 / 1000: loss 2.089761\n",
      "iteration 400 / 1000: loss 1.956986\n",
      "iteration 500 / 1000: loss 1.783870\n",
      "iteration 600 / 1000: loss 1.788431\n",
      "iteration 700 / 1000: loss 1.675544\n",
      "iteration 800 / 1000: loss 1.800275\n",
      "iteration 900 / 1000: loss 1.747724\n",
      "For parameters:  [ 0.1 80.  50. ]\n",
      "Validation accuracy:  0.389\n",
      "iteration 0 / 1000: loss 2.424740\n",
      "iteration 100 / 1000: loss 2.076541\n",
      "iteration 200 / 1000: loss 1.857389\n",
      "iteration 300 / 1000: loss 1.913411\n",
      "iteration 400 / 1000: loss 1.768074\n",
      "iteration 500 / 1000: loss 1.808753\n",
      "iteration 600 / 1000: loss 1.815828\n",
      "iteration 700 / 1000: loss 1.783271\n",
      "iteration 800 / 1000: loss 1.657926\n",
      "iteration 900 / 1000: loss 1.786941\n",
      "For parameters:  [  0.1 100.  100. ]\n",
      "Validation accuracy:  0.388\n",
      "iteration 0 / 1000: loss 2.414725\n",
      "iteration 100 / 1000: loss 2.075634\n",
      "iteration 200 / 1000: loss 1.901399\n",
      "iteration 300 / 1000: loss 1.885077\n",
      "iteration 400 / 1000: loss 1.822682\n",
      "iteration 500 / 1000: loss 1.898381\n",
      "iteration 600 / 1000: loss 1.792772\n",
      "iteration 700 / 1000: loss 1.765559\n",
      "iteration 800 / 1000: loss 1.855042\n",
      "iteration 900 / 1000: loss 1.734327\n",
      "For parameters:  [1.0e-01 1.2e+02 2.0e+02]\n",
      "Validation accuracy:  0.407\n",
      "iteration 0 / 1000: loss 2.351040\n",
      "iteration 100 / 1000: loss 2.266038\n",
      "iteration 200 / 1000: loss 2.194641\n",
      "iteration 300 / 1000: loss 2.204253\n",
      "iteration 400 / 1000: loss 2.172737\n",
      "iteration 500 / 1000: loss 2.078930\n",
      "iteration 600 / 1000: loss 2.101879\n",
      "iteration 700 / 1000: loss 1.967918\n",
      "iteration 800 / 1000: loss 1.987544\n",
      "iteration 900 / 1000: loss 1.879330\n",
      "For parameters:  [ 0.05 20.   50.  ]\n",
      "Validation accuracy:  0.329\n",
      "iteration 0 / 1000: loss 2.370049\n",
      "iteration 100 / 1000: loss 2.168831\n",
      "iteration 200 / 1000: loss 2.093210\n",
      "iteration 300 / 1000: loss 2.053266\n",
      "iteration 400 / 1000: loss 2.101964\n",
      "iteration 500 / 1000: loss 1.908137\n",
      "iteration 600 / 1000: loss 1.999922\n",
      "iteration 700 / 1000: loss 1.909557\n",
      "iteration 800 / 1000: loss 1.847564\n",
      "iteration 900 / 1000: loss 1.904145\n",
      "For parameters:  [5.e-02 5.e+01 1.e+02]\n",
      "Validation accuracy:  0.351\n",
      "iteration 0 / 1000: loss 2.412317\n",
      "iteration 100 / 1000: loss 2.198026\n",
      "iteration 200 / 1000: loss 2.095225\n",
      "iteration 300 / 1000: loss 1.976668\n",
      "iteration 400 / 1000: loss 1.941050\n",
      "iteration 500 / 1000: loss 1.910940\n",
      "iteration 600 / 1000: loss 1.878348\n",
      "iteration 700 / 1000: loss 1.938718\n",
      "iteration 800 / 1000: loss 1.935688\n",
      "iteration 900 / 1000: loss 1.807334\n",
      "For parameters:  [5.e-02 8.e+01 2.e+02]\n",
      "Validation accuracy:  0.352\n",
      "iteration 0 / 1000: loss 2.486944\n",
      "iteration 100 / 1000: loss 2.256717\n",
      "iteration 200 / 1000: loss 2.028991\n",
      "iteration 300 / 1000: loss 1.982665\n",
      "iteration 400 / 1000: loss 2.100781\n",
      "iteration 500 / 1000: loss 1.986462\n",
      "iteration 600 / 1000: loss 1.942680\n",
      "iteration 700 / 1000: loss 1.698867\n",
      "iteration 800 / 1000: loss 1.729140\n",
      "iteration 900 / 1000: loss 1.840234\n",
      "For parameters:  [5.e-02 1.e+02 5.e+01]\n",
      "Validation accuracy:  0.38\n",
      "iteration 0 / 1000: loss 2.315558\n",
      "iteration 100 / 1000: loss 2.128884\n",
      "iteration 200 / 1000: loss 2.012528\n",
      "iteration 300 / 1000: loss 2.068834\n",
      "iteration 400 / 1000: loss 1.972363\n",
      "iteration 500 / 1000: loss 1.995456\n",
      "iteration 600 / 1000: loss 1.882271\n",
      "iteration 700 / 1000: loss 1.860277\n",
      "iteration 800 / 1000: loss 1.784486\n",
      "iteration 900 / 1000: loss 1.893442\n",
      "For parameters:  [5.0e-02 1.2e+02 1.0e+02]\n",
      "Validation accuracy:  0.374\n",
      "iteration 0 / 1000: loss 2.332475\n",
      "iteration 100 / 1000: loss 2.225142\n",
      "iteration 200 / 1000: loss 2.162154\n",
      "iteration 300 / 1000: loss 2.135651\n",
      "iteration 400 / 1000: loss 2.100617\n",
      "iteration 500 / 1000: loss 2.048210\n",
      "iteration 600 / 1000: loss 2.094557\n",
      "iteration 700 / 1000: loss 1.985015\n",
      "iteration 800 / 1000: loss 2.026365\n",
      "iteration 900 / 1000: loss 1.992307\n",
      "For parameters:  [5.e-02 2.e+01 2.e+02]\n",
      "Validation accuracy:  0.293\n",
      "iteration 0 / 1000: loss 2.334120\n",
      "iteration 100 / 1000: loss 2.162681\n",
      "iteration 200 / 1000: loss 2.091840\n",
      "iteration 300 / 1000: loss 2.089505\n",
      "iteration 400 / 1000: loss 1.941406\n",
      "iteration 500 / 1000: loss 1.974974\n",
      "iteration 600 / 1000: loss 1.919954\n",
      "iteration 700 / 1000: loss 2.042265\n",
      "iteration 800 / 1000: loss 1.930522\n",
      "iteration 900 / 1000: loss 1.939204\n",
      "For parameters:  [ 0.05 50.   50.  ]\n",
      "Validation accuracy:  0.353\n",
      "iteration 0 / 1000: loss 2.384247\n",
      "iteration 100 / 1000: loss 2.216204\n",
      "iteration 200 / 1000: loss 2.162714\n",
      "iteration 300 / 1000: loss 1.987644\n",
      "iteration 400 / 1000: loss 2.003775\n",
      "iteration 500 / 1000: loss 1.977043\n",
      "iteration 600 / 1000: loss 1.948022\n",
      "iteration 700 / 1000: loss 1.917032\n",
      "iteration 800 / 1000: loss 1.861673\n",
      "iteration 900 / 1000: loss 1.855129\n",
      "For parameters:  [5.e-02 8.e+01 1.e+02]\n",
      "Validation accuracy:  0.358\n",
      "iteration 0 / 1000: loss 2.456170\n",
      "iteration 100 / 1000: loss 2.143607\n",
      "iteration 200 / 1000: loss 2.061057\n",
      "iteration 300 / 1000: loss 2.011322\n",
      "iteration 400 / 1000: loss 1.956127\n",
      "iteration 500 / 1000: loss 1.946235\n",
      "iteration 600 / 1000: loss 1.872850\n",
      "iteration 700 / 1000: loss 1.808731\n",
      "iteration 800 / 1000: loss 1.926650\n",
      "iteration 900 / 1000: loss 1.799540\n",
      "For parameters:  [5.e-02 1.e+02 2.e+02]\n",
      "Validation accuracy:  0.372\n",
      "iteration 0 / 1000: loss 2.358424\n",
      "iteration 100 / 1000: loss 2.158301\n",
      "iteration 200 / 1000: loss 2.082831\n",
      "iteration 300 / 1000: loss 1.996793\n",
      "iteration 400 / 1000: loss 1.883716\n",
      "iteration 500 / 1000: loss 2.097584\n",
      "iteration 600 / 1000: loss 1.791184\n",
      "iteration 700 / 1000: loss 1.875459\n",
      "iteration 800 / 1000: loss 1.991684\n",
      "iteration 900 / 1000: loss 1.684003\n",
      "For parameters:  [5.0e-02 1.2e+02 5.0e+01]\n",
      "Validation accuracy:  0.364\n",
      "iteration 0 / 1000: loss 2.426748\n",
      "iteration 100 / 1000: loss 2.212196\n",
      "iteration 200 / 1000: loss 2.153461\n",
      "iteration 300 / 1000: loss 2.140821\n",
      "iteration 400 / 1000: loss 2.109336\n",
      "iteration 500 / 1000: loss 2.098734\n",
      "iteration 600 / 1000: loss 2.096055\n",
      "iteration 700 / 1000: loss 2.009370\n",
      "iteration 800 / 1000: loss 1.954278\n",
      "iteration 900 / 1000: loss 2.041460\n",
      "For parameters:  [5.e-02 2.e+01 1.e+02]\n",
      "Validation accuracy:  0.309\n",
      "iteration 0 / 1000: loss 2.488710\n",
      "iteration 100 / 1000: loss 2.201617\n",
      "iteration 200 / 1000: loss 2.121595\n",
      "iteration 300 / 1000: loss 2.083578\n",
      "iteration 400 / 1000: loss 2.024399\n",
      "iteration 500 / 1000: loss 1.957256\n",
      "iteration 600 / 1000: loss 1.956924\n",
      "iteration 700 / 1000: loss 1.985621\n",
      "iteration 800 / 1000: loss 1.922655\n",
      "iteration 900 / 1000: loss 1.849824\n",
      "For parameters:  [5.e-02 5.e+01 2.e+02]\n",
      "Validation accuracy:  0.357\n",
      "iteration 0 / 1000: loss 2.431763\n",
      "iteration 100 / 1000: loss 2.192848\n",
      "iteration 200 / 1000: loss 2.067292\n",
      "iteration 300 / 1000: loss 2.047826\n",
      "iteration 400 / 1000: loss 2.025818\n",
      "iteration 500 / 1000: loss 2.066377\n",
      "iteration 600 / 1000: loss 1.940036\n",
      "iteration 700 / 1000: loss 1.756844\n",
      "iteration 800 / 1000: loss 2.036926\n",
      "iteration 900 / 1000: loss 2.043059\n",
      "For parameters:  [5.e-02 8.e+01 5.e+01]\n",
      "Validation accuracy:  0.369\n",
      "iteration 0 / 1000: loss 2.426608\n",
      "iteration 100 / 1000: loss 2.179206\n",
      "iteration 200 / 1000: loss 2.135026\n",
      "iteration 300 / 1000: loss 2.053648\n",
      "iteration 400 / 1000: loss 1.964938\n",
      "iteration 500 / 1000: loss 1.827273\n",
      "iteration 600 / 1000: loss 1.957723\n",
      "iteration 700 / 1000: loss 1.860583\n",
      "iteration 800 / 1000: loss 1.865474\n",
      "iteration 900 / 1000: loss 1.856841\n",
      "For parameters:  [5.e-02 1.e+02 1.e+02]\n",
      "Validation accuracy:  0.34\n",
      "iteration 0 / 1000: loss 2.322078\n",
      "iteration 100 / 1000: loss 2.165835\n",
      "iteration 200 / 1000: loss 2.043643\n",
      "iteration 300 / 1000: loss 2.044443\n",
      "iteration 400 / 1000: loss 1.978318\n",
      "iteration 500 / 1000: loss 1.970712\n",
      "iteration 600 / 1000: loss 1.925278\n",
      "iteration 700 / 1000: loss 1.862245\n",
      "iteration 800 / 1000: loss 1.861564\n",
      "iteration 900 / 1000: loss 1.921277\n",
      "For parameters:  [5.0e-02 1.2e+02 2.0e+02]\n",
      "Validation accuracy:  0.37\n",
      "iteration 0 / 1000: loss 2.410755\n",
      "iteration 100 / 1000: loss 2.254633\n",
      "iteration 200 / 1000: loss 2.202615\n",
      "iteration 300 / 1000: loss 2.233503\n",
      "iteration 400 / 1000: loss 2.187810\n",
      "iteration 500 / 1000: loss 2.161803\n",
      "iteration 600 / 1000: loss 2.165336\n",
      "iteration 700 / 1000: loss 2.112059\n",
      "iteration 800 / 1000: loss 2.169692\n",
      "iteration 900 / 1000: loss 2.045631\n",
      "For parameters:  [3.e-02 2.e+01 5.e+01]\n",
      "Validation accuracy:  0.296\n",
      "iteration 0 / 1000: loss 2.517343\n",
      "iteration 100 / 1000: loss 2.243674\n",
      "iteration 200 / 1000: loss 2.225400\n",
      "iteration 300 / 1000: loss 2.159400\n",
      "iteration 400 / 1000: loss 2.138322\n",
      "iteration 500 / 1000: loss 2.140139\n",
      "iteration 600 / 1000: loss 2.076396\n",
      "iteration 700 / 1000: loss 2.086987\n",
      "iteration 800 / 1000: loss 2.005066\n",
      "iteration 900 / 1000: loss 1.967004\n",
      "For parameters:  [3.e-02 5.e+01 1.e+02]\n",
      "Validation accuracy:  0.327\n",
      "iteration 0 / 1000: loss 2.584331\n",
      "iteration 100 / 1000: loss 2.214143\n",
      "iteration 200 / 1000: loss 2.159015\n",
      "iteration 300 / 1000: loss 2.138768\n",
      "iteration 400 / 1000: loss 2.062011\n",
      "iteration 500 / 1000: loss 1.975079\n",
      "iteration 600 / 1000: loss 1.978128\n",
      "iteration 700 / 1000: loss 2.004669\n",
      "iteration 800 / 1000: loss 1.929064\n",
      "iteration 900 / 1000: loss 1.901959\n",
      "For parameters:  [3.e-02 8.e+01 2.e+02]\n",
      "Validation accuracy:  0.324\n",
      "iteration 0 / 1000: loss 2.355964\n",
      "iteration 100 / 1000: loss 2.176187\n",
      "iteration 200 / 1000: loss 2.122950\n",
      "iteration 300 / 1000: loss 2.063921\n",
      "iteration 400 / 1000: loss 1.967118\n",
      "iteration 500 / 1000: loss 2.067608\n",
      "iteration 600 / 1000: loss 1.831401\n",
      "iteration 700 / 1000: loss 1.958748\n",
      "iteration 800 / 1000: loss 1.961925\n",
      "iteration 900 / 1000: loss 1.914812\n",
      "For parameters:  [3.e-02 1.e+02 5.e+01]\n",
      "Validation accuracy:  0.342\n",
      "iteration 0 / 1000: loss 2.432296\n",
      "iteration 100 / 1000: loss 2.212569\n",
      "iteration 200 / 1000: loss 2.073832\n",
      "iteration 300 / 1000: loss 2.054449\n",
      "iteration 400 / 1000: loss 2.021154\n",
      "iteration 500 / 1000: loss 1.895779\n",
      "iteration 600 / 1000: loss 2.001518\n",
      "iteration 700 / 1000: loss 2.004096\n",
      "iteration 800 / 1000: loss 1.989022\n",
      "iteration 900 / 1000: loss 1.958868\n",
      "For parameters:  [3.0e-02 1.2e+02 1.0e+02]\n",
      "Validation accuracy:  0.347\n",
      "iteration 0 / 1000: loss 2.410058\n",
      "iteration 100 / 1000: loss 2.245920\n",
      "iteration 200 / 1000: loss 2.223053\n",
      "iteration 300 / 1000: loss 2.203342\n",
      "iteration 400 / 1000: loss 2.159479\n",
      "iteration 500 / 1000: loss 2.133978\n",
      "iteration 600 / 1000: loss 2.130835\n",
      "iteration 700 / 1000: loss 2.098385\n",
      "iteration 800 / 1000: loss 2.140647\n",
      "iteration 900 / 1000: loss 2.082727\n",
      "For parameters:  [3.e-02 2.e+01 2.e+02]\n",
      "Validation accuracy:  0.274\n",
      "iteration 0 / 1000: loss 2.278432\n",
      "iteration 100 / 1000: loss 2.213406\n",
      "iteration 200 / 1000: loss 2.208855\n",
      "iteration 300 / 1000: loss 2.066700\n",
      "iteration 400 / 1000: loss 2.051404\n",
      "iteration 500 / 1000: loss 2.077448\n",
      "iteration 600 / 1000: loss 2.030642\n",
      "iteration 700 / 1000: loss 1.890243\n",
      "iteration 800 / 1000: loss 2.013143\n",
      "iteration 900 / 1000: loss 1.944813\n",
      "For parameters:  [3.e-02 5.e+01 5.e+01]\n",
      "Validation accuracy:  0.323\n",
      "iteration 0 / 1000: loss 2.359923\n",
      "iteration 100 / 1000: loss 2.203286\n",
      "iteration 200 / 1000: loss 2.120148\n",
      "iteration 300 / 1000: loss 2.125394\n",
      "iteration 400 / 1000: loss 2.053189\n",
      "iteration 500 / 1000: loss 1.995812\n",
      "iteration 600 / 1000: loss 2.018539\n",
      "iteration 700 / 1000: loss 2.050029\n",
      "iteration 800 / 1000: loss 1.940142\n",
      "iteration 900 / 1000: loss 1.933720\n",
      "For parameters:  [3.e-02 8.e+01 1.e+02]\n",
      "Validation accuracy:  0.334\n",
      "iteration 0 / 1000: loss 2.405881\n",
      "iteration 100 / 1000: loss 2.213258\n",
      "iteration 200 / 1000: loss 2.137062\n",
      "iteration 300 / 1000: loss 2.156366\n",
      "iteration 400 / 1000: loss 2.055444\n",
      "iteration 500 / 1000: loss 1.980912\n",
      "iteration 600 / 1000: loss 1.989552\n",
      "iteration 700 / 1000: loss 2.006236\n",
      "iteration 800 / 1000: loss 1.967418\n",
      "iteration 900 / 1000: loss 1.890400\n",
      "For parameters:  [3.e-02 1.e+02 2.e+02]\n",
      "Validation accuracy:  0.342\n",
      "iteration 0 / 1000: loss 2.374897\n",
      "iteration 100 / 1000: loss 2.138628\n",
      "iteration 200 / 1000: loss 2.068674\n",
      "iteration 300 / 1000: loss 2.125135\n",
      "iteration 400 / 1000: loss 2.040380\n",
      "iteration 500 / 1000: loss 1.963412\n",
      "iteration 600 / 1000: loss 1.904206\n",
      "iteration 700 / 1000: loss 1.920922\n",
      "iteration 800 / 1000: loss 1.934204\n",
      "iteration 900 / 1000: loss 1.865893\n",
      "For parameters:  [3.0e-02 1.2e+02 5.0e+01]\n",
      "Validation accuracy:  0.355\n",
      "iteration 0 / 1000: loss 2.496609\n",
      "iteration 100 / 1000: loss 2.246303\n",
      "iteration 200 / 1000: loss 2.158703\n",
      "iteration 300 / 1000: loss 2.165596\n",
      "iteration 400 / 1000: loss 2.142796\n",
      "iteration 500 / 1000: loss 2.118159\n",
      "iteration 600 / 1000: loss 2.097155\n",
      "iteration 700 / 1000: loss 2.030318\n",
      "iteration 800 / 1000: loss 2.074157\n",
      "iteration 900 / 1000: loss 2.019326\n",
      "For parameters:  [3.e-02 2.e+01 1.e+02]\n",
      "Validation accuracy:  0.306\n",
      "iteration 0 / 1000: loss 2.427046\n",
      "iteration 100 / 1000: loss 2.260295\n",
      "iteration 200 / 1000: loss 2.188190\n",
      "iteration 300 / 1000: loss 2.178611\n",
      "iteration 400 / 1000: loss 2.136770\n",
      "iteration 500 / 1000: loss 2.090595\n",
      "iteration 600 / 1000: loss 2.073835\n",
      "iteration 700 / 1000: loss 2.018430\n",
      "iteration 800 / 1000: loss 2.010283\n",
      "iteration 900 / 1000: loss 2.006048\n",
      "For parameters:  [3.e-02 5.e+01 2.e+02]\n",
      "Validation accuracy:  0.327\n",
      "iteration 0 / 1000: loss 2.552201\n",
      "iteration 100 / 1000: loss 2.187638\n",
      "iteration 200 / 1000: loss 2.124987\n",
      "iteration 300 / 1000: loss 2.150177\n",
      "iteration 400 / 1000: loss 2.011614\n",
      "iteration 500 / 1000: loss 2.083815\n",
      "iteration 600 / 1000: loss 1.995669\n",
      "iteration 700 / 1000: loss 2.041478\n",
      "iteration 800 / 1000: loss 1.878195\n",
      "iteration 900 / 1000: loss 1.901249\n",
      "For parameters:  [3.e-02 8.e+01 5.e+01]\n",
      "Validation accuracy:  0.346\n",
      "iteration 0 / 1000: loss 2.352675\n",
      "iteration 100 / 1000: loss 2.162777\n",
      "iteration 200 / 1000: loss 2.103423\n",
      "iteration 300 / 1000: loss 2.114047\n",
      "iteration 400 / 1000: loss 2.057731\n",
      "iteration 500 / 1000: loss 1.995899\n",
      "iteration 600 / 1000: loss 2.036591\n",
      "iteration 700 / 1000: loss 1.996298\n",
      "iteration 800 / 1000: loss 1.986193\n",
      "iteration 900 / 1000: loss 1.962722\n",
      "For parameters:  [3.e-02 1.e+02 1.e+02]\n",
      "Validation accuracy:  0.354\n",
      "iteration 0 / 1000: loss 2.339715\n",
      "iteration 100 / 1000: loss 2.198587\n",
      "iteration 200 / 1000: loss 2.154298\n",
      "iteration 300 / 1000: loss 2.091658\n",
      "iteration 400 / 1000: loss 2.037219\n",
      "iteration 500 / 1000: loss 2.012031\n",
      "iteration 600 / 1000: loss 1.972614\n",
      "iteration 700 / 1000: loss 1.979341\n",
      "iteration 800 / 1000: loss 1.894481\n",
      "iteration 900 / 1000: loss 1.896944\n",
      "For parameters:  [3.0e-02 1.2e+02 2.0e+02]\n",
      "Validation accuracy:  0.334\n",
      "The best combination of parameters is: learning rate =  0.1 , hidden layer size =  80 , batch size =  200\n",
      "For this combination, the validation accuracy is  0.414\n"
     ]
    }
   ],
   "source": [
    "best_3layer_sigmoid = None # store the best model into this \n",
    "\n",
    "#################################################################################\n",
    "# TODO: Tune hyperparameters using the validation set. Store your best trained  #\n",
    "# model in best_3layer_sigmoid.                                                 #\n",
    "#################################################################################\n",
    "\n",
    "#Initialize \"best\" validation accuracy\n",
    "\n",
    "best_val_acc = 0\n",
    "\n",
    "#Initialize vector to store all validation accuracies\n",
    "\n",
    "val_acc_vec = np.zeros(len(parameters_matrix))\n",
    "\n",
    "#Cycle through the matrix of parameters\n",
    "\n",
    "for i in range(len(parameters_matrix)):\n",
    "\n",
    "  input_size = 32 * 32 * 3\n",
    "  num_layers = 3\n",
    "  hidden_size = int(parameters_matrix[i,1])\n",
    "  hidden_sizes = [hidden_size]*(num_layers-1)\n",
    "  num_classes = 10\n",
    "  learning_rate = parameters_matrix[i,0]\n",
    "  learning_rate_decay = 0.95\n",
    "  batch_size = int(parameters_matrix[i,2])\n",
    "  net = NeuralNetwork(input_size, hidden_sizes, num_classes, num_layers, nonlinearity='sigmoid')\n",
    "\n",
    "  # Train the network\n",
    "  stats = net.train(X_train, y_train, X_val, y_val,\n",
    "              num_iters=1000, batch_size=batch_size,\n",
    "              learning_rate=learning_rate, learning_rate_decay=learning_rate_decay,\n",
    "              reg=0.0, verbose=True)\n",
    "  \n",
    "  # Predict on the validation set\n",
    "  val_acc = (net.predict(X_val) == y_val).mean()\n",
    "\n",
    "  print('For parameters: ', parameters_matrix[i])\n",
    "  print('Validation accuracy: ', val_acc)\n",
    "\n",
    "  #Store the validaction accuracies\n",
    "\n",
    "  val_acc_vec[i] = val_acc\n",
    "\n",
    "  #Save the best model and stats\n",
    "\n",
    "  if val_acc > best_val_acc:\n",
    "\n",
    "    best_val_acc = val_acc\n",
    "    best_3layer_sigmoid = net\n",
    "    best_stats = stats\n",
    "\n",
    "#Print the best combination of trial parameters:\n",
    "\n",
    "ind = np.argmax(val_acc_vec)\n",
    "\n",
    "print('The best combination of parameters is: learning rate = ',parameters_matrix[ind,0], ', hidden layer size = ',int(parameters_matrix[ind,1]), ', batch size = ',int(parameters_matrix[ind,2]))\n",
    "print('For this combination, the validation accuracy is ', val_acc_vec[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmGnorklVMfm"
   },
   "source": [
    "# Run on the test set\n",
    "When you are done experimenting, you should evaluate your final trained networks on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 508,
     "status": "ok",
     "timestamp": 1601351927646,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "kgGl5e99VMfm",
    "outputId": "5607c61d-01a6-4752-fcaa-5f2c02afeee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-layer relu\n",
      "Test accuracy:  0.5038\n"
     ]
    }
   ],
   "source": [
    "print('Two-layer relu')\n",
    "test_acc = (best_2layer_relu.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1601352675661,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "64v7WnUMVMfo",
    "outputId": "b8c41512-c4e3-4bd1-81e1-a1aa532860d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-layer sigmoid\n",
      "Test accuracy:  0.4354\n"
     ]
    }
   ],
   "source": [
    "print('Two-layer sigmoid')\n",
    "test_acc = (best_2layer_sigmoid.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1601353185877,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "cXwmDZjNVMfq",
    "outputId": "8fea0b78-2811-4317-e8ed-06e6a066e127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three-layer relu\n",
      "Test accuracy:  0.505\n"
     ]
    }
   ],
   "source": [
    "print('Three-layer relu')\n",
    "test_acc = (best_3layer_relu.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1601353853772,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "VqYrc1n-VMft",
    "outputId": "86051239-6d3a-4c9d-b669-3294cd750af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three-layer sigmoid\n",
      "Test accuracy:  0.3964\n"
     ]
    }
   ],
   "source": [
    "print('Three-layer sigmoid')\n",
    "test_acc = (best_3layer_sigmoid.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Z9lj96fVMfv"
   },
   "source": [
    "# Kaggle output\n",
    "\n",
    "Once you are satisfied with your solution and test accuracy output a file to submit your test set predictions to the Kaggle for Assignment 2 Neural Network. Use the following code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1092,
     "status": "ok",
     "timestamp": 1601352025340,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "OHfzDJRLVMfv"
   },
   "outputs": [],
   "source": [
    "output_submission_csv('nn_2layer_relu_submission.csv', best_2layer_relu.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1236,
     "status": "ok",
     "timestamp": 1601352694730,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "OGuGOQQ3VMfy"
   },
   "outputs": [],
   "source": [
    "output_submission_csv('nn_2layer_sigmoid_submission.csv', best_2layer_sigmoid.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 533,
     "status": "ok",
     "timestamp": 1601343177787,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "CqEKSKcRVMf0"
   },
   "outputs": [],
   "source": [
    "output_submission_csv('nn_3layer_relu_submission.csv', best_3layer_relu.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1601343178909,
     "user": {
      "displayName": "Oswaldo Russián",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg48C1P8EDo4KoxIFghJem9iasGJTnavYuciGhxEg=s64",
      "userId": "02791438348892449348"
     },
     "user_tz": 300
    },
    "id": "HvS6JXYuVMf2"
   },
   "outputs": [],
   "source": [
    "output_submission_csv('nn_3layer_sigmoid_submission.csv', best_3layer_sigmoid.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "neural_network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
